\IEEEPARstart{E}{motions} play and increasing role in our interactions with people in our everyday lives. Over the last decade, the use and interaction with computers has increased immensely. Humans desperately rely on computers for a vast majority of jobs, to communicate with friends, or to interact with intelligent virtual assistants to control smart homes and answer questions. Additionally, an increasing number of jobs have been converted from requiring a human to a robot or other computer-based machine. Humans are interacting with computers more than ever before and demanding computers to be more efficient, user friendly, intelligent and help in more intricate ways. However, computers are relatively limited in their capacity of helping humans without the ability to recognize human emotions. 

Humans rely on recognizing how people express their emotions whether they use verbal or nonverbal communication. Nonverbal communication expressed through body language such as facial expressions, body gestures, or eye contact is found to contain a majority of the information communicated during conversation. Albert Mehrabian 's body language research in 1971 concluded that 55\% of communication is nonverbal, 38\% is vocal and only 7\% is the words that are spoken \cite{mehrabian-1971}. This more insightful understanding of human communication can aid the development of human-computer interaction (HCI).

An extremely popular field of research within HCI is Human Emotion Recognition or sometimes referred to as computational empathy. There have been drastic improvements to intelligent agents recognizing human emotions with the primary motivation of creating relational robots. However, perceiving human emotion has far more applications than socially intelligent robots. Recognizing emotions can be applied to education, driver safety, software engineering, website customization, gaming, and more \cite{vinola-2015}\cite{ko≈Çakowska-2014}. Human emotion is an exceedingly complex combination of channels of information, typically referred to as modalities, but most researchers reduce this exhaustive list of into larger categories of emotion recognition: Facial Expression Recognition (FER), Body Expression Recognition, Physiological Signal Processing, and Speech Emotion Recognition (SER) \cite{saxena-2020}\cite{hatem-2022}\cite{heredia-2022}.

Each of these aspects of HER provides pertinent insight to how humans express emotions. Each component has been heavily researched and models are now achieving accuracy of over 95\%. Despite the successful prediction accuracy of these models, the scope of their application to real-world problems are fairly limited due to the representativeness of the data the models are trained on. However, if all of these models are used together to provide a more holistic view of human emotions, it is possible to create flexible models that can handle the ambiguity of human emotions in real-world data. This survey aims to summarize the individual components of human emotion recognition to motivate the need for future research of multimodal models. However, the primary objective of this survey is to compare and contrast previous multimodal approaches to emotion recognition to provide insight to more robust models.  