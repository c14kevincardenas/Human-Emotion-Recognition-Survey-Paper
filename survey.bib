
@article{saxena_emotion_2020,
	title = {Emotion {Recognition} and {Detection} {Methods}: {A} {Comprehensive} {Survey}},
	volume = {2},
	issn = {2642-2859},
	shorttitle = {Emotion {Recognition} and {Detection} {Methods}},
	url = {https://iecscience.org/jpapers/46#abstract},
	doi = {10.33969/AIS.2020.21005},
	abstract = {Human emotion recognition through artificial intelligence is one of the most popular research fields among researchers nowadays. The fields of Human Computer Interaction (HCI) and Affective Computing are being extensively used to sense human emotions. Humans generally use a lot of indirect and non-verbal means to convey their emotions. The presented exposition aims to provide an overall overview with the analysis of all the noteworthy emotion detection methods at a single location. To the best of our knowledge, this is the first attempt to outline all the emotion recognition models developed in the last decade. The paper is comprehended by expending more than hundred papers; a detailed analysis of the methodologies along with the datasets is carried out in the paper. The study revealed that emotion detection is predominantly carried out through four major methods, namely, facial expression recognition, physiological signals recognition, speech signals variation and text semantics on standard databases such as JAFFE, CK+, Berlin Emotional Database, SAVEE, etc. as well as self-generated databases. Generally seven basic emotions are recognized through these methods. Further, we have compared different methods employed for emotion detection in humans. The best results were obtained by using Stationary Wavelet Transform for Facial Emotion Recognition , Particle Swarm Optimization assisted Biogeography based optimization algorithms for emotion recognition through speech, Statistical features coupled with different methods for physiological signals, Rough set theory coupled with SVM for text semantics with respective accuracies of 98.83\%,99.47\%, 87.15\%,87.02\% . Overall, the method of Particle Swarm Optimization assisted Biogeography based optimization algorithms with an accuracy of 99.47\% on BES dataset gave the best results.},
	language = {en},
	number = {1},
	urldate = {2022-09-30},
	journal = {Journal of Artificial Intelligence and Systems},
	author = {Saxena, Anvita and Khanna, Ashish and Gupta, Deepak},
	month = feb,
	year = {2020},
	note = {Publisher: Institute of Electronics and Computer},
	pages = {53--79},
	file = {Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\C7GV7FA9\\Saxena1 et al. - 2020 - Emotion Recognition and Detection Methods A Compr.pdf:application/pdf;Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\GKIGTTDV\\46.html:text/html},
}

@article{vinola_survey_2015,
	title = {A {Survey} on {Human} {Emotion} {Recognition} {Approaches}, {Databases} and {Applications}},
	volume = {14},
	issn = {1577-5097},
	url = {http://elcvia.cvc.uab.es/article/view/v14-n2-vinola-vimaladevi},
	doi = {10.5565/rev/elcvia.795},
	abstract = {This paper presents the various emotion classification and recognition systems which implement methods aiming at improving Human Machine Interaction. The modalities and approaches used for affect detection vary and contribute to accuracy and efficacy in detecting emotions of human beings. This paper discovers them in a comparison and descriptive manner. Various applications that use the methodologies in different contexts to address the challenges in real time are discussed. This survey also describes the databases that can be used as standard data sets in the process of emotion identification. Thus an integrated discussion of methods, databases used and applications pertaining to the emerging field of Affective Computing (AC) is done and surveyed.},
	language = {en},
	number = {2},
	urldate = {2022-10-11},
	journal = {ELCVIA Electronic Letters on Computer Vision and Image Analysis},
	author = {Vinola, C. and Vimaladevi, K.},
	month = dec,
	year = {2015},
	pages = {24},
	file = {Vinola and Vimaladevi - 2015 - A Survey on Human Emotion Recognition Approaches, .pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\MH7EEUKU\\Vinola and Vimaladevi - 2015 - A Survey on Human Emotion Recognition Approaches, .pdf:application/pdf},
}

@incollection{hippe_emotion_2014,
	address = {Cham},
	title = {Emotion {Recognition} and {Its} {Applications}},
	volume = {300},
	isbn = {978-3-319-08490-9 978-3-319-08491-6},
	url = {http://link.springer.com/10.1007/978-3-319-08491-6_5},
	abstract = {This paper aims at illustrating diversity of possible emotion recognition applications. It provides concise review of affect recognition methods based on different inputs such as biometrics, video channel or behavioral data. It proposes a set of research scenarios of emotion recognition applications in the following domains: software engineering, website customization, education, and gaming. The scenarios show complexity and problems of applying affective computing in different domains. Analysis of the scenarios allows drawing some conclusions on challenges of automatic recognition that have to be addressed by further research.},
	language = {en},
	urldate = {2022-10-11},
	booktitle = {Human-{Computer} {Systems} {Interaction}: {Backgrounds} and {Applications} 3},
	publisher = {Springer International Publishing},
	author = {Kołakowska, A. and Landowska, A. and Szwoch, M. and Szwoch, W. and Wróbel, M. R.},
	editor = {Hippe, Zdzisław S. and Kulikowski, Juliusz L. and Mroczek, Teresa and Wtorek, Jerzy},
	year = {2014},
	doi = {10.1007/978-3-319-08491-6_5},
	note = {Series Title: Advances in Intelligent Systems and Computing},
	pages = {51--62},
	file = {Kołakowska et al. - 2014 - Emotion Recognition and Its Applications.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\XXKUVK69\\Kołakowska et al. - 2014 - Emotion Recognition and Its Applications.pdf:application/pdf},
}

@inproceedings{abimala_comprehensive_2023,
	address = {Singapore},
	series = {Algorithms for {Intelligent} {Systems}},
	title = {A {Comprehensive} {Study} on {Automatic} {Emotion} {Detection} {System} {Using} {EEG} {Signals} and {Deep} {Learning} {Algorithms}},
	isbn = {978-981-19212-6-1},
	doi = {10.1007/978-981-19-2126-1_21},
	abstract = {Emotion refers to a person’s current mental and cognitive condition. It demonstrates that it plays a dynamic function in communication, decision-making, and physical health. The development of a human emotion recognition system shifted the focus of research in a variety of fields, including cognitive science, computer science, psychology, neuroscience, and artificial intelligence. Emotion recognition is a subset of the brain–computer interface (BCI). Speech, facial expressions, and EEG data all can be used to determine emotions. EEG waves have shown to be an excellent choice for automatic emotion recognition because they cannot be faked like speech or facial expressions. The purpose of this work is to conduct a survey of emotion recognition using deep learning techniques. The benefits of employing deep learning algorithms in the extraction and categorization of EEG signals are investigated. This study could pave the way for more research into the usage of deep learning techniques to EEG-based emotion recognition systems.},
	language = {en},
	booktitle = {Proceedings of {International} {Conference} on {Computational} {Intelligence}},
	publisher = {Springer Nature},
	author = {Abimala, T. and Narmadha, T. V. and Raamesh, Lilly},
	editor = {Tiwari, Ritu and Pavone, Mario F. and Ravindranathan Nair, Ranjith},
	year = {2023},
	keywords = {Artificial intelligence, Deep learning, Emotion recognition, Feature extraction, BCI, Cognitive science, EEG, Neuroscience},
	pages = {267--282},
	annote = {“EEG waves have shown to be an excellent choice for automatic emotion recognition because they cannot be faked like speech or facial expressions.”
},
}

@article{dzedzickis_human_2020,
	title = {Human {Emotion} {Recognition}: {Review} of {Sensors} and {Methods}},
	volume = {20},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	shorttitle = {Human {Emotion} {Recognition}},
	url = {https://www.mdpi.com/1424-8220/20/3/592},
	doi = {10.3390/s20030592},
	abstract = {Automated emotion recognition (AEE) is an important issue in various fields of activities which use human emotional reactions as a signal for marketing, technical equipment, or human–robot interaction. This paper analyzes scientific research and technical papers for sensor use analysis, among various methods implemented or researched. This paper covers a few classes of sensors, using contactless methods as well as contact and skin-penetrating electrodes for human emotion detection and the measurement of their intensity. The results of the analysis performed in this paper present applicable methods for each type of emotion and their intensity and propose their classification. The classification of emotion sensors is presented to reveal area of application and expected outcomes from each method, as well as their limitations. This paper should be relevant for researchers using human emotion evaluation and analysis, when there is a need to choose a proper method for their purposes or to find alternative decisions. Based on the analyzed human emotion recognition sensors and methods, we developed some practical applications for humanizing the Internet of Things (IoT) and affective computing systems.},
	language = {en},
	number = {3},
	urldate = {2022-10-27},
	journal = {Sensors},
	author = {Dzedzickis, Andrius and Kaklauskas, Artūras and Bucinskas, Vytautas},
	month = jan,
	year = {2020},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {emotion perception, human emotions, physiologic sensors},
	pages = {592},
	annote = {This is not AI based. Simply looking at EEG signals and emotions

“Figure 1. Russel’s circumplex model of emotions.” (Dzedzickis et al., 2020, p. 2)
},
	file = {Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\7DER63AB\\Dzedzickis et al. - 2020 - Human Emotion Recognition Review of Sensors and M.pdf:application/pdf;Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\SHWIY8YB\\htm.html:text/html},
}

@article{margaret_survey_2022,
	title = {A survey on brain computer interface using {EEG} signals for emotion recognition},
	volume = {2518},
	issn = {0094-243X},
	url = {https://aip.scitation.org/doi/abs/10.1063/5.0103476},
	doi = {10.1063/5.0103476},
	number = {1},
	urldate = {2022-10-27},
	journal = {AIP Conference Proceedings},
	author = {Margaret, M. Jehosheba and Banu, N. M. Masoodhu},
	month = sep,
	year = {2022},
	note = {Publisher: American Institute of Physics},
	pages = {040002},
	annote = {Uses preprocessing, feature extraction and ML
},
	file = {Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\929DIX3A\\Margaret and Banu - 2022 - A survey on brain computer interface using EEG sig.pdf:application/pdf},
}

@inproceedings{shenoy_self_2022,
	title = {A {Self} {Learning} {System} for {Emotion} {Awareness} and {Adaptation} in {Humanoid} {Robots}},
	doi = {10.1109/RO-MAN53752.2022.9900581},
	abstract = {Humanoid robots provide a unique opportunity for personalized interaction using emotion recognition. However, emotion recognition performed by humanoid robots in complex social interactions is limited in the flexibility of interaction as well as personalization and adaptation in the responses. We designed an adaptive learning system for real-time emotion recognition that elicits its own ground-truth data and updates individualized models to improve performance over time. A Convolutional Neural Network based on off-the-shelf ResNet50 and Inception v3 are assembled to form an ensemble model which is used for real-time emotion recognition through facial expression. Two sets of robot behaviors, general and personalized, are developed to evoke different emotion responses. The personalized behaviors are adapted based on user preferences collected through a pre-test survey. The performance of the proposed system is verified through a 2-stage user study and tested for the accuracy of the self-supervised retraining. We also evaluate the effectiveness of the personalized behavior of the robot in evoking intended emotions between stages using trust, empathy and engagement scales. The participants are divided into two groups based on their familiarity and previous interactions with the robot. The results of emotion recognition indicate a 12\% increase in the F1 score for 7 emotions in stage 2 compared to pre-trained model. Higher mean scores for trust, engagement, and empathy are observed in both participant groups. The average similarity score for both stages was 82\% and the average success rate of eliciting the intended emotion increased by 8.28\% between stages, despite their differences in familiarity thus offering a way to mitigate novelty effect patterns among user interactions.},
	booktitle = {2022 31st {IEEE} {International} {Conference} on {Robot} and {Human} {Interactive} {Communication} ({RO}-{MAN})},
	author = {Shenoy, Sudhir and Jiang, Yusheng and Lynch, Tyler and Manuel, Lauren Isabelle and Doryab, Afsaneh},
	month = aug,
	year = {2022},
	note = {ISSN: 1944-9437},
	keywords = {deep learning, Emotion recognition, Humanoid robots, Transfer learning, Adaptation models, Adaptive learning, Data models, emotion recognition, engagement, Real-time systems, social robot, user-adaptive},
	pages = {912--919},
	annote = {Cool application of using HER for Human Robot Interaction
},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DFCS-General\\Zotero\\storage\\LNB7ACTD\\9900581.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\ZUMGIHHB\\Shenoy et al. - 2022 - A Self Learning System for Emotion Awareness and A.pdf:application/pdf},
}

@inproceedings{tyagi_review_2022,
	title = {A {Review} {And} {Analysis} of {Emotion} {Based} {Harmful} {Speech} {Detection} {Using} {Machine} {And} {Deep} {Learning} {And} {Future} {Direction}},
	doi = {10.1109/ICCC202255925.2022.9922704},
	abstract = {Nowadays, harmful speech is among the most difficult issues deal by internet. Hand-reporting comments and posts suspected of harmful speech on the internet is a frequent method of dealing with web suspects of harmful speech. This has many drawbacks. Because it necessitates the involvement of a human, this process takes a long time. Corpora of speech signals, a wide range of speech attributes, and systems for recognizing emotions are all included in this collection, which examines the most available research on speech emotion recognition. This paper presents the review of some extraordinary works done in this field. Furthermore, a short analysis is over the machine learning and deep learning algorithms. The state vector machine algorithm found to be very useful in the speech emotion recognition technique. The novel analysisand survey found to be very useful in for the upgraded researches. The paper also touched on the necessity of using a variety of classification models. If a future study is to be done on emotion recognition in general or on hazardous speech detection in particular, certain difficulties have been noted.},
	booktitle = {2022 {IEEE} 10th {Jubilee} {International} {Conference} on {Computational} {Cybernetics} and {Cyber}-{Medical} {Systems} ({ICCC})},
	author = {Tyagi, Suryakant and Szénási, Sándor},
	month = jul,
	year = {2022},
	keywords = {Deep learning, Emotion recognition, Face recognition, Feature extraction, Classifiers, Databases, Deep neural network, Machine learning algorithms, MFCC, Ml \& DL approaches, Speech Database, Speech emotion recognition, System performance, Voice activity detection},
	pages = {000089--000094},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DFCS-General\\Zotero\\storage\\SA4AX8HE\\9922704.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\ZB39Z44Y\\Tyagi and Szénási - 2022 - A Review And Analysis of Emotion Based Harmful Spe.pdf:application/pdf},
}

@article{cabitza_unbearable_2022,
	title = {The unbearable (technical) unreliability of automated facial emotion recognition},
	volume = {9},
	issn = {2053-9517},
	url = {https://doi.org/10.1177/20539517221129549},
	doi = {10.1177/20539517221129549},
	abstract = {Emotion recognition, and in particular acial emotion recognition (FER), is among the most controversial applications of machine learning, not least because of its ethical implications for human subjects. In this article, we address the controversial conjecture that machines can read emotions from our facial expressions by asking whether this task can be performed reliably. This means, rather than considering the potential harms or scientific soundness of facial emotion recognition systems, focusing on the reliability of the ground truths used to develop emotion recognition systems, assessing how well different human observers agree on the emotions they detect in subjects? faces. Additionally, we discuss the extent to which sharing context can help observers agree on the emotions they perceive on subjects? faces. Briefly, we demonstrate that when large and heterogeneous samples of observers are involved, the task of emotion detection from static images crumbles into inconsistency. We thus reveal that any endeavour to understand human behaviour from large sets of labelled patterns is over-ambitious, even if it were technically feasible. We conclude that we cannot speak of actual accuracy for facial emotion recognition systems for any practical purposes.},
	language = {en},
	number = {2},
	urldate = {2022-10-27},
	journal = {Big Data \& Society},
	author = {Cabitza, Federico and Campagner, Andrea and Mattioli, Martina},
	month = jul,
	year = {2022},
	note = {Publisher: SAGE Publications Ltd},
	pages = {20539517221129549},
	annote = {FER cannot be the only thing we use for HER!
},
	file = {SAGE PDF Full Text:C\:\\Users\\DFCS-General\\Zotero\\storage\\LKXVA944\\Cabitza et al. - 2022 - The unbearable (technical) unreliability of automa.pdf:application/pdf},
}

@inproceedings{kant_emotion_2023,
	address = {Singapore},
	series = {Lecture {Notes} in {Networks} and {Systems}},
	title = {Emotion {Recognition} in {Human} {Face} {Through} {Video} {Surveillance}—{A} {Survey} of {State}-of-the-{Art} {Approaches}},
	isbn = {978-981-19009-5-2},
	doi = {10.1007/978-981-19-0095-2_6},
	abstract = {Emotion is a state of thoughts and feelings related to events and happenings. Emotion plays a significant role in all aspects of life. If we capture the emotion of an individual, then many issues can be resolved. Emotion recognition is becoming an interesting field of research nowadays due to huge amount of information available from various communication platforms. Emotion detection will play vital role as we are moving towards digital era from all the aspects of life. The capacity to understand the emotion by computer is necessary in many applications, especially emotion detected from video. At the present, emotional factors are significant because we get efficient aspect of customer behaviour. Video surveillance plays important role in recent times for face detection and feature extraction. Video surveillance will help us to understand the emotion that is being carried out by an individual. Facial expression is the natural aspect which helps to integrate the quality of emotion. Human face helps to get the emotion a person expressing in due course of action and activities. Many clues we can get from the human face which will help us to formulate and solve the issues and threat in the domain of emotion detection. This scholarly work is a comparative study of emotion recognition and classification of emotion from video sequences which exhibits the current trends and challenges in emotion recognition. Intensity of emotion and duration of an emotion are the two significant attributes for effective identification of emotion of an individual though video surveillance. This study of emotion recognition has given insight and future direction in the domain of emotion recognition technology.},
	language = {en},
	booktitle = {Information and {Communication} {Technology} for {Competitive} {Strategies} ({ICTCS} 2021)},
	publisher = {Springer Nature},
	author = {Kant, Krishna and Shah, D. B.},
	editor = {Joshi, Amit and Mahmud, Mufti and Ragel, Roshan G.},
	year = {2023},
	keywords = {Emotion recognition, Feature extraction, Emotion classification, Emotion intensity, Human face detection, Video surveillance},
	pages = {49--59},
}

@article{siddiqui_survey_2022,
	title = {A {Survey} on {Databases} for {Multimodal} {Emotion} {Recognition} and an {Introduction} to the {VIRI} ({Visible} and {InfraRed} {Image}) {Database}},
	volume = {6},
	issn = {2414-4088},
	url = {https://www.mdpi.com/2414-4088/6/6/47},
	doi = {10.3390/mti6060047},
	abstract = {Multimodal human–computer interaction (HCI) systems pledge a more human–human-like interaction between machines and humans. Their prowess in emanating an unambiguous information exchange between the two makes these systems more reliable, efﬁcient, less error prone, and capable of solving complex tasks. Emotion recognition is a realm of HCI that follows multimodality to achieve accurate and natural results. The prodigious use of affective identiﬁcation in e-learning, marketing, security, health sciences, etc., has increased demand for high-precision emotion recognition systems. Machine learning (ML) is getting its feet wet to ameliorate the process by tweaking the architectures or wielding high-quality databases (DB). This paper presents a survey of such DBs that are being used to develop multimodal emotion recognition (MER) systems. The survey illustrates the DBs that contain multi-channel data, such as facial expressions, speech, physiological signals, body movements, gestures, and lexical features. Few unimodal DBs are also discussed that work in conjunction with other DBs for affect recognition. Further, VIRI, a new DB of visible and infrared (IR) images of subjects expressing ﬁve emotions in an uncontrolled, real-world environment, is presented. A rationale for the superiority of the presented corpus over the existing ones is instituted.},
	language = {en},
	number = {6},
	urldate = {2022-10-27},
	journal = {Multimodal Technologies and Interaction},
	author = {Siddiqui, Mohammad Faridul Haque and Dhakal, Parashar and Yang, Xiaoli and Javaid, Ahmad Y.},
	month = jun,
	year = {2022},
	pages = {47},
	annote = {This article focuses on the complexity of databases for multi-modal HER 
},
	file = {Siddiqui et al. - 2022 - A Survey on Databases for Multimodal Emotion Recog.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\7TTU246A\\Siddiqui et al. - 2022 - A Survey on Databases for Multimodal Emotion Recog.pdf:application/pdf},
}

@article{zong_adapting_2022,
	title = {Adapting {Multiple} {Distributions} for {Bridging} {Emotions} from {Different} {Speech} {Corpora}},
	volume = {24},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/24/9/1250},
	doi = {10.3390/e24091250},
	abstract = {In this paper, we focus on a challenging, but interesting, task in speech emotion recognition (SER), i.e., cross-corpus SER. Unlike conventional SER, a feature distribution mismatch may exist between the labeled source (training) and target (testing) speech samples in cross-corpus SER because they come from different speech emotion corpora, which degrades the performance of most well-performing SER methods. To address this issue, we propose a novel transfer subspace learning method called multiple distribution-adapted regression (MDAR) to bridge the gap between speech samples from different corpora. Specifically, MDAR aims to learn a projection matrix to build the relationship between the source speech features and emotion labels. A novel regularization term called multiple distribution adaption (MDA), consisting of a marginal and two conditional distribution-adapted operations, is designed to collaboratively enable such a discriminative projection matrix to be applicable to the target speech samples, regardless of speech corpus variance. Consequently, by resorting to the learned projection matrix, we are able to predict the emotion labels of target speech samples when only the source label information is given. To evaluate the proposed MDAR method, extensive cross-corpus SER tasks based on three different speech emotion corpora, i.e., EmoDB, eNTERFACE, and CASIA, were designed. Experimental results showed that the proposed MDAR outperformed most recent state-of-the-art transfer subspace learning methods and even performed better than several well-performing deep transfer learning methods in dealing with cross-corpus SER tasks.},
	language = {en},
	number = {9},
	urldate = {2022-10-27},
	journal = {Entropy},
	author = {Zong, Yuan and Lian, Hailun and Chang, Hongli and Lu, Cheng and Tang, Chuangao},
	month = sep,
	year = {2022},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {cross-corpus speech emotion recognition, domain adaptation, speech emotion recognition, subspace learning, transfer learning},
	pages = {1250},
	file = {Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\DGHJNI49\\Zong et al. - 2022 - Adapting Multiple Distributions for Bridging Emoti.pdf:application/pdf;Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\RJUY88V4\\1250.html:text/html},
}

@inproceedings{ebdali_takalloo_overview_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Networks} and {Systems}},
	title = {An {Overview} of {Emotion} {Recognition} from {Body} {Movement}},
	isbn = {978-3-031-08812-4},
	doi = {10.1007/978-3-031-08812-4_11},
	abstract = {Understanding human emotions has become a popular research and practical topic in recent years. There are many research works that have good results on detecting human emotions from facial expressions, speech and text. Recognizing emotions from body posture or movement is an emerging area of research, and it has shown progressive results. This brief survey presents an overview of recent research in this field. The relationship between emotions and body movements is discussed. The factors that affect this relation are presented. Based on recent advanced research, an integrated and comprehensive process for the automatic detection of emotions based on body movements is introduced. Each component of this process is considered. In particular, body movement models, their evaluation, and available datasets are examined.},
	language = {en},
	booktitle = {Complex, {Intelligent} and {Software} {Intensive} {Systems}},
	publisher = {Springer International Publishing},
	author = {Ebdali Takalloo, Laleh and Li, Kin Fun and Takano, Kosuke},
	editor = {Barolli, Leonard},
	year = {2022},
	keywords = {Emotion recognition, Body movement},
	pages = {105--117},
	file = {Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\S6M436K5\\Ebdali Takalloo et al. - 2022 - An Overview of Emotion Recognition from Body Movem.pdf:application/pdf},
}

@article{miracle_survey_nodate,
	title = {A {Survey} on {Data} {Fusion} {Methods} for {Multimodal} {Emotion} {Recognition}},
	abstract = {Several approaches have been employed to recognize human emotions based on facial expression, speech visuals or physiological signals. It is even more difficult to fuse these three modalities together in order to improve the accuracy and robustness of the emotion recognition system. In this paper, we present a survey on the different types of data fusion methods used for multimodal emotion recognition. We explore three fusion models namely early fusion, late fusion and hybrid fusion.},
	language = {en},
	author = {Miracle, Udurume and Lim, Wansu},
	pages = {2},
	annote = {How to fuse data together. 
},
	file = {Miracle and Lim - A Survey on Data Fusion Methods for Multimodal Emo.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\7MXAI79M\\Miracle and Lim - A Survey on Data Fusion Methods for Multimodal Emo.pdf:application/pdf},
}

@incollection{kumar_emerging_2022,
	address = {Singapore},
	title = {Emerging {Features} and {Classification} {Algorithms} for {Speaker} {Emotion} {Recognition}: {A} {Survey}},
	volume = {828},
	isbn = {9789811679841 9789811679858},
	shorttitle = {Emerging {Features} and {Classification} {Algorithms} for {Speaker} {Emotion} {Recognition}},
	url = {https://link.springer.com/10.1007/978-981-16-7985-8_93},
	abstract = {In the world of Artiﬁcial Intelligence, human machine interaction plays a signiﬁcant role. We want to make our machine to behave more like humans. Emotion recognition system helps us to recognize human emotions. Emotions plays important role in behavior of a person as research says that most of the actions performed by human are based on his or her emotional state. In this paper we are trying to Summarizes recent works in the ﬁeld of emotion recognition. Survey is conducted for factors like use of speaker speciﬁc features, different classiﬁcation methods used for classiﬁcation of emotions. It also studies what are the results of use of different techniques. Survey shows that with increase in number of features used and combination of different classiﬁcation techniques improves rate of recognition. Some emotions like sad and fear are having less accuracy rate for classiﬁcation.},
	language = {en},
	urldate = {2022-10-27},
	booktitle = {{ICCCE} 2021},
	publisher = {Springer Nature Singapore},
	author = {Jagtap, Shilpa and Mali, Suresh},
	editor = {Kumar, Amit and Mozar, Stefan},
	year = {2022},
	doi = {10.1007/978-981-16-7985-8_93},
	note = {Series Title: Lecture Notes in Electrical Engineering},
	pages = {879--884},
	annote = {Just a survey.

Concludes with “Speaker emotion recognition (SER) is made of different stages such as preprocessing of speech signal, feature extraction, classification”
},
	file = {Jagtap and Mali - 2022 - Emerging Features and Classification Algorithms fo.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\3QQ5LZM5\\Jagtap and Mali - 2022 - Emerging Features and Classification Algorithms fo.pdf:application/pdf},
}

@article{hameed_survey_2022,
	title = {A {Survey} for {Emotion} {Recognition} {Based} on {Speech} {Signal}},
	volume = {14},
	copyright = {Copyright (c) 2022 Fatima A. Hameed, Dr. Loay E. Georgeb},
	issn = {2521-3504},
	url = {http://www.qu.edu.iq/journalcm/index.php/journalcm/article/view/905},
	doi = {10.29304/jqcm.2022.14.1.905},
	abstract = {There are many characteristics of human beings, such as fingerprints, DNA, and retinal pigmentation that are essential. A person's voice is unique to each individual. Humans use speech to communicate their thoughts and feelings. The process of determining one's mental state involves expressing one's basic emotions in words. A person's emotions play a significant part in his or her daily existence. In order to convey one's thoughts and feelings to others, it is essential to use this method. Emotions can be discerned from speech information because humans have a built-in ability to do so. The selection of an emotion recognition body (speech database), the identification of numerous variables connected to speech, and the selection of a suitable classification model are the main hurdles for emotion recognition. To identify emotions, an emotion identification system analyzes auditory structural elements of speech. The analysis is based on multiple research papers and includes an in-depth examination of the methodology and data sets. The study discovered that emotion detection is accomplished using four distinct methodologies: physiological signal recognition, facial expression recognition, a variety of speech signals, and text semantics in both objective and subjective databases such as JAFFE, CK+, Berlin emotional database, and SAVEE. In general, these techniques enable the identification of seven basic emotions. To determine the emotion, the audio expression for eight emotions (happy, angry, sad, depressed, bored, anxious, afraid, and apprehensive), all published research maintain an average level of accuracy. The major goal of this survey is to compare and contrast numerous previous survey methodologies, which are backed up by empirical evidence. This study covered signal collection processing, feature extraction, and signal classification, as well as the pros and downsides of each approach. It also goes over a number of strategies that may need to be tweaked at each step of speech emotion recognition.},
	language = {en},
	number = {1},
	urldate = {2022-10-27},
	journal = {Journal of Al-Qadisiyah for computer science and mathematics},
	author = {Hameed, Fatima A. and Georgeb, Dr Loay E.},
	month = apr,
	year = {2022},
	note = {Number: 1},
	keywords = {Feature extraction, Audio Speech Characteristics, classification, Emotional speech databases, Speech Analysis},
	pages = {Page64--73},
	annote = {Surveys recent speech emotion recognition.
Highlights classifers, databases, and more. 
},
	file = {Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\URLN42MT\\Hameed and Georgeb - 2022 - A Survey for Emotion Recognition Based on Speech S.pdf:application/pdf},
}

@article{kang_emotion_2022,
	title = {Emotion {Recognition} using {Short}-{Term} {Multi}-{Physiological} {Signals}},
	volume = {16},
	issn = {1976-7277},
	url = {https://koreascience.kr/article/JAKO202211563854034.page},
	doi = {10.3837/tiis.2022.03.018},
	abstract = {Technology for emotion recognition is an essential part of human personality analysis. To define human personality characteristics, the existing method used the survey method. However, there are many cases where communication cannot make without considering emotions. Hence, emotional recognition technology is an essential element for communication but has also been adopted in many other fields. A person's emotions are revealed in various ways, typically including facial, speech, and biometric responses. Therefore, various methods can recognize emotions, e.g., images, voice signals, and physiological signals. Physiological signals are measured with biological sensors and analyzed to identify emotions. This study employed two sensor types. First, the existing method, the binary arousal-valence method, was subdivided into four levels to classify emotions in more detail. Then, based on the current techniques classified as High/Low, the model was further subdivided into multi-levels. Finally, signal characteristics were extracted using a 1-D Convolution Neural Network (CNN) and classified sixteen feelings. Although CNN was used to learn images in 2D, sensor data in 1D was used as the input in this paper. Finally, the proposed emotional recognition system was evaluated by measuring actual sensors.},
	language = {eng},
	number = {3},
	urldate = {2022-10-27},
	journal = {KSII Transactions on Internet and Information Systems (TIIS)},
	author = {Kang, Tae-Koo},
	year = {2022},
	note = {Publisher: Korean Society for Internet Information},
	pages = {1076--1094},
	annote = {EMG signals and PPG signals.

CNN Model performed 92\% (EMG and PPG)  
16 Emotions! 
Frequently cited Russel “Russell proposed a two-dimensional emotional model that divided emotions along the arousal and valence axes to express emotions quantitatively.”
},
	file = {Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\I5NI49HI\\Kang - 2022 - Emotion Recognition using Short-Term Multi-Physiol.pdf:application/pdf;Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\QAX27HVM\\JAKO202211563854034.html:text/html},
}

@book{mehrabian_nonverbal_2017,
	title = {Nonverbal {Communication}},
	isbn = {978-1-351-30871-7},
	abstract = {Even though our society subtly discourages the verbal expression of emotions, most of us, in ostensibly conforming to our roles, nevertheless manage to express likes, dislikes, status differences, personalities, as well as weaknesses in nonverbal ways. Using vocal expressions; gestures, postures, and movements, we amplify, restrict, or deny what our words say to one another, and even say some things with greater facility and efficiency than with words.In this new, multidimensional approach to the subject of nonverbal communication Albert Mehrabian brings together a great deal of original work which includes descriptions of new experimental methods that are especially suited to this field, detailed findings of studies scattered throughout the literature, and most importantly, the integration of these findings within a compact framework.The framework starts with the analysis of the meanings of various nonverbal behaviors and is based on the fact that more than half of the variance in the significance of nonverbal signals can be described in terms of the three orthogonal dimensions of positiveness, potency or status, and responsiveness. These three dimensions not only constitute the semantic space for nonverbal communication, but also help to identify groups of behaviors relating to each, to describe characteristic differences in nonverbal communication, to analyze and generate rules for the understanding of inconsistent messages, and to provide researchers with new and comprehensive measures for description of social behavior.This volume will be particularly valuable for both the professional psychologist and the graduate student in psychology. It will also be of great interest to professionals in the fields of speech and communication, sociology, anthropology, and psychiatry.},
	language = {en},
	publisher = {Routledge},
	author = {Mehrabian, Albert},
	month = jul,
	year = {2017},
	note = {Google-Books-ID: KlAPEAAAQBAJ},
	keywords = {Psychology / General, Psychology / Social Psychology},
	annote = {non-verbal correspondence is the most common type, with manner of speaking and body language accounting for 38 \% and 55 \% of emotional information, in both, while the remaining 7\% is made up of spoken words (the "38 \%– 55 \%–7 \% rule").
},
}

@article{hatem_information_2022,
	title = {The {Information} {Channels} of {Emotion} {Recognition}: {A} {Review}},
	volume = {19},
	shorttitle = {The {Information} {Channels} of {Emotion} {Recognition}},
	doi = {10.14704/WEB/V19I1/WEB19064},
	abstract = {Humans are emotional beings. When we express about emotions, we frequently use several modalities, whether we want to so overtly (i.e., Speech, facial expressions,..) or implicitly (i.e., body language, text,..). Emotion recognition has lately piqued the interest of many researchers, and various techniques have been studied. A review on emotion recognition is given in this article. The survey seeks single and multiple source of data or information channels that may be utilized to identify emotions and includes a literature analysis on current studies published to each information channel, as well as the techniques employed and the findings obtained. Ultimately, some of the present emotion recognition problems and future work recommendations have been mentioned.},
	journal = {Webology},
	author = {Hatem, Ahmed and Al-Bakry, Abbas},
	month = jan,
	year = {2022},
	annote = {This is VERY close to the survey paper I want to write…
Summarizes recent uni-moda models (for speech and facial recognition) and 6 multi-modal models [2015-2021]. FER [2017-2021]. Speech [2015-2021].
Talks about how to join data
},
}

@article{tsai_enhancing_2022,
	title = {Enhancing the accuracy of a human emotion recognition method using spatial temporal graph convolutional networks},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-022-13653-x},
	doi = {10.1007/s11042-022-13653-x},
	abstract = {Artificial intelligence technology has been widely used in human emotion recognition applications. Unlike traditional facial, semantic and brain wave technology, spatio-temporal graph convolution network technology has been shown to be useful for human emotional feature training and recognition, but is limited by the loss of subtle features in the training process caused by convolutional neural network technology. As a result, the use of spatio-temporal graph convolution network technology to identify human emotional categories has been restricted. This research paper aims to improve the accuracy of human emotion recognition methods based on spatio-temporal graphs, and uses human skeleton keypoint detection technology to calculate the degree of face swing and variation. These are used as classification features for human emotion, and are supplemented with the classification results from a spatio-temporal graph convolution network recognition model after performing a weight analysis, in order to obtain a high level of accuracy for human emotion identification. This research paper proposes an approach to the enhancement of human emotion recognition accuracy to import activities change recognition ability, efficiently identify multi-level human emotional status to strengthen artificial emotional intelligence calculation. In this research paper, we use human skeleton keypoint information as the basis for an emotion recognition framework. We carry out model training on the keypoints of the human face based on the specific degree of emotional change. Our proposed method is compared with related approaches in the literature (ST-GCN, 2S-AGCN and GCN-NAS), and we find that the accuracy of emotion recognition can be improved by 17.71\%, 18.75\% and 16.67\%, respectively.},
	language = {en},
	urldate = {2022-10-27},
	journal = {Multimedia Tools and Applications},
	author = {Tsai, Ming-Fong and Chen, Chiung-Hung},
	month = aug,
	year = {2022},
	keywords = {Human emotion recognition, Human skeleton Keypoint detection, Spatial temporal graph convolution networks},
	annote = {Uses facial orientation
},
	file = {Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\STWJ6IMF\\Tsai and Chen - 2022 - Enhancing the accuracy of a human emotion recognit.pdf:application/pdf},
}

@misc{ahmad_survey_2022,
	title = {A {Survey} on {Physiological} {Signal} {Based} {Emotion} {Recognition}},
	url = {http://arxiv.org/abs/2205.10466},
	abstract = {Physiological Signals are the most reliable form of signals for emotion recognition, as they cannot be controlled deliberately by the subject. Existing review papers on emotion recognition based on physiological signals surveyed only the regular steps involved in the workflow of emotion recognition such as preprocessing, feature extraction, and classification. While these are important steps, such steps are required for any signal processing application. Emotion recognition poses its own set of challenges that are very important to address for a robust system. Thus, to bridge the gap in the existing literature, in this paper, we review the effect of inter-subject data variance on emotion recognition, important data annotation techniques for emotion recognition and their comparison, data preprocessing techniques for each physiological signal, data splitting techniques for improving the generalization of emotion recognition models and different multimodal fusion techniques and their comparison. Finally we discuss key challenges and future directions in this field.},
	urldate = {2022-10-27},
	publisher = {arXiv},
	author = {Ahmad, Zeeshan and Khan, Naimul},
	month = may,
	year = {2022},
	note = {arXiv:2205.10466 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	annote = {Most recent survey on physiological signal HER
},
	file = {arXiv Fulltext PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\QD4DG2TX\\Ahmad and Khan - 2022 - A Survey on Physiological Signal Based Emotion Rec.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\YZ6YBGKG\\2205.html:text/html},
}

@misc{franceschini_multimodal_2022,
	title = {Multimodal {Emotion} {Recognition} with {Modality}-{Pairwise} {Unsupervised} {Contrastive} {Loss}},
	url = {http://arxiv.org/abs/2207.11482},
	abstract = {Emotion recognition is involved in several real-world applications. With an increase in available modalities, automatic understanding of emotions is being performed more accurately. The success in Multimodal Emotion Recognition (MER), primarily relies on the supervised learning paradigm. However, data annotation is expensive, time-consuming, and as emotion expression and perception depends on several factors (e.g., age, gender, culture) obtaining labels with a high reliability is hard. Motivated by these, we focus on unsupervised feature learning for MER. We consider discrete emotions, and as modalities text, audio and vision are used. Our method, as being based on contrastive loss between pairwise modalities, is the first attempt in MER literature. Our end-to-end feature learning approach has several differences (and advantages) compared to existing MER methods: i) it is unsupervised, so the learning is lack of data labelling cost; ii) it does not require data spatial augmentation, modality alignment, large number of batch size or epochs; iii) it applies data fusion only at inference; and iv) it does not require backbones pre-trained on emotion recognition task. The experiments on benchmark datasets show that our method outperforms several baseline approaches and unsupervised learning methods applied in MER. Particularly, it even surpasses a few supervised MER state-of-the-art.},
	urldate = {2022-10-27},
	publisher = {arXiv},
	author = {Franceschini, Riccardo and Fini, Enrico and Beyan, Cigdem and Conti, Alessandro and Arrigoni, Federica and Ricci, Elisa},
	month = jul,
	year = {2022},
	note = {arXiv:2207.11482 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
	annote = {Unsupervised learning to remove labelling cost, no need for data spatial augmentation/modality alignment/large number of batch size and epochs, does not requie backbones pre-trained on ER task
},
	file = {arXiv Fulltext PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\5C753G77\\Franceschini et al. - 2022 - Multimodal Emotion Recognition with Modality-Pairw.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\5UHL5UXX\\2207.html:text/html},
}

@article{heredia_adaptive_2022,
	title = {Adaptive {Multimodal} {Emotion} {Detection} {Architecture} for {Social} {Robots}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3149214},
	abstract = {Emotion recognition is a strategy for social robots used to implement better Human-Robot Interaction and model their social behaviour. Since human emotions can be expressed in different ways (e.g., face, gesture, voice), multimodal approaches are useful to support the recognition process. However, although there exist studies dealing with multimodal emotion recognition for social robots, they still present limitations in the fusion process, dropping their performance if one or more modalities are not present or if modalities have different qualities. This is a common situation in social robotics, due to the high variety of the sensory capacities of robots; hence, more flexible multimodal models are needed. In this context, we propose an adaptive and flexible emotion recognition architecture able to work with multiple sources and modalities of information and manage different levels of data quality and missing data, to lead robots to better understand the mood of people in a given environment and accordingly adapt their behaviour. Each modality is analyzed independently to then aggregate the partial results with a previous proposed fusion method, called EmbraceNet+, which is adapted and integrated to our proposed framework. We also present an extensive review of state-of-the-art studies dealing with fusion methods for multimodal emotion recognition approaches. We evaluate the performance of our proposed architecture by performing different tests in which several modalities are combined to classify emotions using four categories (i.e., happiness, neutral, sadness, and anger). Results reveal that our approach is able to adapt to the quality and presence of modalities. Furthermore, results obtained are validated and compared with other similar proposals, obtaining competitive performance with state-of-the-art models.},
	journal = {IEEE Access},
	author = {Heredia, Juanpablo and Lopes-Silva, Edmundo and Cardinale, Yudith and Diaz-Amado, Jose and Dongo, Irvin and Graterol, Wilfredo and Aguilera, Ana},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {social robots, Emotion recognition, Face recognition, Adaptation models, fusion process, Hidden Markov models, multimodal models, Robot sensing systems, Speech recognition, Videos},
	pages = {20727--20744},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DFCS-General\\Zotero\\storage\\ISER3N6I\\9705576.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\963L2VNE\\Heredia et al. - 2022 - Adaptive Multimodal Emotion Detection Architecture.pdf:application/pdf},
}

@article{middya_deep_2022,
	title = {Deep learning based multimodal emotion recognition using model-level fusion of audio–visual modalities},
	volume = {244},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122002593},
	doi = {10.1016/j.knosys.2022.108580},
	abstract = {Emotion identification based on multimodal data (e.g., audio, video, text, etc.) is one of the most demanding and important research fields, with various uses. In this context, this research work has conducted a rigorous exploration of model-level fusion to find out the optimal multimodal model for emotion recognition using audio and video modalities. More specifically, separate novel feature extractor networks for audio and video data are proposed. After that, an optimal multimodal emotion recognition model is created by fusing audio and video features at the model level. The performances of the proposed models are assessed based on two benchmark multimodal datasets namely Ryerson Audio–Visual Database of Emotional Speech and Song (RAVDESS) and Surrey Audio–Visual Expressed Emotion (SAVEE) using various performance metrics. The proposed models achieve high predictive accuracies of 99\% and 86\% on the SAVEE and RAVDESS datasets, respectively. The effectiveness of the models are also verified by comparing their performances with the existing emotion recognition models. Some case studies are also conducted to explore the model’s ability to capture the variability of emotional states of the speakers in publicly available real-world audio–visual media.},
	language = {en},
	urldate = {2022-10-27},
	journal = {Knowledge-Based Systems},
	author = {Middya, Asif Iqbal and Nag, Baibhav and Roy, Sarbani},
	month = may,
	year = {2022},
	keywords = {Deep learning, Audio features, Classification, Multimodal emotion recognition, Video features},
	pages = {108580},
	annote = {Combines Audio and Visual
8 emotion classes: fearful, disgust, angry, sad, happy, calm, neutral, and surprised
facial features: Caffe

},
	file = {ScienceDirect Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\M9FJ3N3U\\S0950705122002593.html:text/html},
}

@article{russell_circumplex_nodate,
	title = {A circumplex model of affect.},
	volume = {39},
	issn = {1939-1315},
	url = {https://psycnet.apa.org/fulltext/1981-25062-001.pdf},
	doi = {10.1037/h0077714},
	number = {6},
	urldate = {2022-11-07},
	journal = {Journal of Personality and Social Psychology},
	author = {Russell, James A.},
	note = {Publisher: US: American Psychological Association},
	pages = {1161},
	file = {Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\KRRNSZMR\\1981-25062-001.html:text/html},
}

@book{scherer_approaches_2014,
	title = {Approaches {To} {Emotion}},
	isbn = {978-1-317-75764-1},
	abstract = {This sourcebook is intended as a reader in the fullest sense of that word: a work that offers researchers and students alike the opportunity to examine the many different aspects and widely divergent approaches to the study of emotion. The contributors include samples of biological, ontogenetic, ethological, psychological, sociological, and anthropological approaches.},
	language = {en},
	publisher = {Psychology Press},
	author = {Scherer, Klaus R. and Ekman, Paul},
	month = may,
	year = {2014},
	note = {Google-Books-ID: k0mhAwAAQBAJ},
	keywords = {Psychology / General, Psychology / Social Psychology},
}

@article{jack_dynamic_2014,
	title = {Dynamic {Facial} {Expressions} of {Emotion} {Transmit} an {Evolving} {Hierarchy} of {Signals} over {Time}},
	volume = {24},
	issn = {09609822},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982213015194},
	doi = {10.1016/j.cub.2013.11.064},
	abstract = {Designed by biological [1, 2] and social [3] evolutionary pressures, facial expressions of emotion comprise speciﬁc facial movements [4–8] to support a near-optimal system of signaling and decoding [9, 10]. Although highly dynamical [11, 12], little is known about the form and function of facial expression temporal dynamics. Do facial expressions transmit diagnostic signals simultaneously to optimize categorization of the six classic emotions, or sequentially to support a more complex communication system of successive categorizations over time? Our data support the latter. Using a combination of perceptual expectation modeling [13–15], information theory [16, 17], and Bayesian classiﬁers, we show that dynamic facial expressions of emotion transmit an evolving hierarchy of ‘‘biologically basic to socially speciﬁc’’ information over time. Early in the signaling dynamics, facial expressions systematically transmit few, biologically rooted face signals [1] supporting the categorization of fewer elementary categories (e.g., approach/avoidance). Later transmissions comprise more complex signals that support categorization of a larger number of socially speciﬁc categories (i.e., the six classic emotions). Here, we show that dynamic facial expressions of emotion provide a sophisticated signaling system, questioning the widely accepted notion that emotion communication is comprised of six basic (i.e., psychologically irreducible) categories [18], and instead suggesting four.},
	language = {en},
	number = {2},
	urldate = {2022-11-07},
	journal = {Current Biology},
	author = {Jack, Rachael E. and Garrod, Oliver G.B. and Schyns, Philippe G.},
	month = jan,
	year = {2014},
	pages = {187--192},
	file = {Jack et al. - 2014 - Dynamic Facial Expressions of Emotion Transmit an .pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\GGHUVSEW\\Jack et al. - 2014 - Dynamic Facial Expressions of Emotion Transmit an .pdf:application/pdf},
}

@article{mansourian_fecal-derived_2016,
	title = {Fecal-{Derived} {Phenol} {Induces} {Egg}-{Laying} {Aversion} in {Drosophila}},
	volume = {26},
	issn = {09609822},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982216308594},
	doi = {10.1016/j.cub.2016.07.065},
	abstract = {Feces is an abundant, rich source of energy, utilized by a myriad of organisms, not least by members of the order Diptera, i.e., ﬂies. How Drosophila melanogaster reacts to fecal matter remains unclear. Here, we examined oviposition behavior toward a range of fecal samples from mammals native to the putative Southeast African homeland of the ﬂy. We show that D. melanogaster display a strong oviposition aversion toward feces from carnivorous mammals but indifference or even attraction toward herbivore dung. We identify a set of four predictor volatiles, which can be used to differentiate fecal from non-fecal matter, as well as separate carnivore from herbivore feces. Of these volatiles, phenol—indicative of carnivore feces—confers egg-laying aversion and is detected by a single class of sensory neurons expressing Or46a. The Or46a-expressing neurons are necessary and sufﬁcient for oviposition site aversion. We further demonstrate that carnivore feces—unlike herbivore dung—contain a high rate of pathogenic bacteria taxa. These harmful bacteria produce phenol from L-tyrosine, an amino acid speciﬁcally enriched in high protein diets, such as consumed by carnivores. Finally, we demonstrate that carnivore feces, as well as phenol, is also avoided by a ball-rolling species of dung beetle, suggesting that phenol is a widespread avoidance signal because of its association with pathogenic bacteria.},
	language = {en},
	number = {20},
	urldate = {2022-11-07},
	journal = {Current Biology},
	author = {Mansourian, Suzan and Corcoran, Jacob and Enjin, Anders and Löfstedt, Christer and Dacke, Marie and Stensmyr, Marcus C.},
	month = oct,
	year = {2016},
	pages = {2762--2769},
	file = {Mansourian et al. - 2016 - Fecal-Derived Phenol Induces Egg-Laying Aversion i.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\QNQYDP7G\\Mansourian et al. - 2016 - Fecal-Derived Phenol Induces Egg-Laying Aversion i.pdf:application/pdf},
}

@article{verma_affect_2017,
	title = {Affect representation and recognition in {3D} continuous valence–arousal–dominance space},
	volume = {76},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-015-3119-y},
	doi = {10.1007/s11042-015-3119-y},
	abstract = {Currently, the focus of research on human affect recognition has shifted from six basic emotions to complex affect recognition in continuous two or three dimensional space due to the following challenges: (i) the difficulty in representing and analyzing large number of emotions in one framework, (ii) the problem of representing complex emotions in the framework, and (iii) the lack of validation of the framework through measured signals, and (iv) the lack of applicability of the selected framework to other aspects of affective computing. This paper presents a Valence – Arousal – Dominance framework to represent emotions. This framework is capable of representing complex emotions on continuous 3D space. To validate the model, an affect recognition technique has been proposed that analyses spontaneous physiological (EEG) and visual cues. The DEAP dataset is a multimodal emotion dataset which contains video and physiological signals as well as Valence, Arousal and Dominance values. This dataset has been used for multimodal analysis and recognition of human emotions. The results prove the correctness and sufficiency of the proposed framework. The model has also been compared with other two dimensional models and the capacity of the model to represent many more complex emotions has been discussed.},
	language = {en},
	number = {2},
	urldate = {2022-11-07},
	journal = {Multimedia Tools and Applications},
	author = {Verma, Gyanendra K and Tiwary, Uma Shanker},
	month = jan,
	year = {2017},
	pages = {2159--2183},
	file = {Verma and Tiwary - 2017 - Affect representation and recognition in 3D contin.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\XP77AGKF\\Verma and Tiwary - 2017 - Affect representation and recognition in 3D contin.pdf:application/pdf},
}

@article{zheng_safety_2016,
	title = {Safety {Needs} {Mediate} {Stressful} {Events} {Induced} {Mental} {Disorders}},
	volume = {2016},
	issn = {2090-5904, 1687-5443},
	url = {https://www.hindawi.com/journals/np/2016/8058093/},
	doi = {10.1155/2016/8058093},
	abstract = {“Safety first,” we say these words almost every day, but we all take this for granted for what Maslow proposed in his famous theory of
              Hierarchy of Needs
              : safety needs come second to physiological needs. Here we propose that safety needs come before physiological needs. Safety needs are personal security, financial security, and health and well-being, which are more fundamental than physiological needs. Safety worrying is the major reason for mental disorders, such as anxiety, phobia, depression, and PTSD. The neural basis for safety is amygdala, LC/NE system, and corticotrophin-releasing hormone system, which can be regarded as a “safety circuitry,” whose major behavior function is “fight or flight” and “fear and anger” emotions. This is similar to the Appraisal theory for emotions: fear is due to the primary appraisal, which is related to safety of individual, while anger is due to secondary appraisal, which is related to coping with the unsafe situations. If coping is good, the individual will be happy; if coping failed, the individual will be sad or depressed.},
	language = {en},
	urldate = {2022-11-07},
	journal = {Neural Plasticity},
	author = {Zheng, Zheng and Gu, Simeng and Lei, Yu and Lu, Shanshan and Wang, Wei and Li, Yang and Wang, Fushun},
	year = {2016},
	pages = {1--6},
	file = {Zheng et al. - 2016 - Safety Needs Mediate Stressful Events Induced Ment.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\JHX6YHSJ\\Zheng et al. - 2016 - Safety Needs Mediate Stressful Events Induced Ment.pdf:application/pdf},
}

@article{gu_neuromodulator_2016,
	title = {Neuromodulator and {Emotion} {Biomarker} for {Stress} {Induced} {Mental} {Disorders}},
	volume = {2016},
	issn = {2090-5904, 1687-5443},
	url = {https://www.hindawi.com/journals/np/2016/2609128/},
	doi = {10.1155/2016/2609128},
	abstract = {Affective disorders are a leading cause of disabilities worldwide, and the etiology of these many affective disorders such as depression and posttraumatic stress disorder is due to hormone changes, which includes hypothalamus-pituitary-adrenal axis in the peripheral nervous system and neuromodulators in the central nervous system. Consistent with pharmacological studies indicating that medical treatment acts by increasing the concentration of catecholamine, the locus coeruleus (LC)/norepinephrine (NE) system is regarded as a critical part of the central “stress circuitry,” whose major function is to induce “fight or flight” behavior and fear and anger emotion. Despite the intensive studies, there is still controversy about NE with fear and anger. For example, the rats with LC ablation were more reluctant to leave a familiar place and took longer to consume the food pellets in an unfamiliar place (neophobia, i.e., fear in response to novelty). The reason for this discrepancy might be that NE is not only for flight (fear), but also for fight (anger). Here, we try to review recent literatures about NE with stress induced emotions and their relations with mental disorders. We propose that stress induced NE release can induce both fear and anger. “Adrenaline rush or norepinephrine rush” and fear and anger emotion might act as biomarkers for mental disorders.},
	language = {en},
	urldate = {2022-11-07},
	journal = {Neural Plasticity},
	author = {Gu, Simeng and Wang, Wei and Wang, Fushun and Huang, Jason H.},
	year = {2016},
	pages = {1--6},
	file = {Gu et al. - 2016 - Neuromodulator and Emotion Biomarker for Stress In.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\PSTGSQE2\\Gu et al. - 2016 - Neuromodulator and Emotion Biomarker for Stress In.pdf:application/pdf},
}

@article{gu_model_2019,
	title = {A {Model} for {Basic} {Emotions} {Using} {Observations} of {Behavior} in {Drosophila}},
	volume = {10},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00781},
	abstract = {Emotion plays a crucial role, both in general human experience and in psychiatric illnesses. Despite the importance of emotion, the relative lack of objective methodologies to scientifically studying emotional phenomena limits our current understanding and thereby calls for the development of novel methodologies, such us the study of illustrative animal models. Analysis of Drosophila and other insects has unlocked new opportunities to elucidate the behavioral phenotypes of fundamentally emotional phenomena. Here we propose an integrative model of basic emotions based on observations of this animal model. The basic emotions are internal states that are modulated by neuromodulators, and these internal states are externally expressed as certain stereotypical behaviors, such as instinct, which is proposed as ancient mechanisms of survival. There are four kinds of basic emotions: happiness, sadness, fear, and anger, which are differentially associated with three core affects: reward (happiness), punishment (sadness), and stress (fear and anger). These core affects are analogous to the three primary colors (red, yellow, and blue) in that they are combined in various proportions to result in more complex “higher order” emotions, such as love and aesthetic emotion. We refer to our proposed model of emotions as called the “Three Primary Color Model of Basic Emotions.”},
	urldate = {2022-11-07},
	journal = {Frontiers in Psychology},
	author = {Gu, Simeng and Wang, Fushun and Patel, Nitesh P. and Bourgeois, James A. and Huang, Jason H.},
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\8AX5BITK\\Gu et al. - 2019 - A Model for Basic Emotions Using Observations of B.pdf:application/pdf},
}

@book{chollet_deep_2021,
	title = {Deep {Learning} with {Python}, {Second} {Edition}},
	isbn = {978-1-63835-009-5},
	abstract = {Unlock the groundbreaking advances of deep learning with this extensively revised edition of the bestselling original. Learn directly from the creator of Keras and master practical Python deep learning techniques that are easy to apply in the real world.In Deep Learning with Python, Second Edition you will learn:  Deep learning from first principles Image classification \& image segmentation Timeseries forecasting Text classification and machine translation Text generation, neural style transfer, and image generation  Deep Learning with Python has taught thousands of readers how to put the full capabilities of deep learning into action. This extensively revised second edition introduces deep learning using Python and Keras, and is loaded with insights for both novice and experienced ML practitioners. You’ll learn practical techniques that are easy to apply in the real world, and important theory for perfecting neural networks.  Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.  About the technology Recent innovations in deep learning unlock exciting new software capabilities like automated language translation, image recognition, and more. Deep learning is becoming essential knowledge for every software developer, and modern tools like Keras and TensorFlow put it within your reach, even if you have no background in mathematics or data science.   About the book Deep Learning with Python, Second Edition introduces the field of deep learning using Python and the powerful Keras library. In this new edition, Keras creator François Chollet offers insights for both novice and experienced machine learning practitioners. As you move through this book, you’ll build your understanding through intuitive explanations, crisp illustrations, and clear examples. You’ll pick up the skills to start developing deep-learning applications.  What's inside  Deep learning from first principles Image classification and image segmentation Time series forecasting Text classification and machine translation Text generation, neural style transfer, and image generation  About the reader For readers with intermediate Python skills. No previous experience with Keras, TensorFlow, or machine learning is required.  About the author François Chollet is a software engineer at Google and creator of the Keras deep-learning library.  Table of Contents 1 What is deep learning? 2 The mathematical building blocks of neural networks 3 Introduction to Keras and TensorFlow 4 Getting started with neural networks: Classification and regression 5 Fundamentals of machine learning 6 The universal workflow of machine learning 7 Working with Keras: A deep dive 8 Introduction to deep learning for computer vision 9 Advanced deep learning for computer vision 10 Deep learning for timeseries 11 Deep learning for text 12 Generative deep learning 13 Best practices for the real world 14 Conclusions},
	language = {en},
	publisher = {Simon and Schuster},
	author = {Chollet, Francois},
	month = dec,
	year = {2021},
	note = {Google-Books-ID: mjVKEAAAQBAJ},
	keywords = {Computers / Data Science / Machine Learning, Computers / Data Science / Neural Networks, Computers / Languages / Python},
}

@article{ge_facial_2022,
	title = {Facial expression recognition based on deep learning},
	volume = {215},
	issn = {01692607},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169260722000062},
	doi = {10.1016/j.cmpb.2022.106621},
	abstract = {Background and objective: Facial expression recognition technology will play an increasingly important role in our daily life. Autonomous driving, virtual reality and all kinds of robots integrated into our life depend on the development of facial expression recognition technology. Many tasks in the ﬁeld of computer vision are based on deep learning technology and convolutional neural network. The paper proposes an occluded expression recognition model based on the generated countermeasure network. The model is divided into two modules, namely, occluded face image restoration and face recognition.
Methods: Firstly, this paper summarizes the research status of deep facial expression recognition methods in recent ten years and the development of related facial expression database. Then, the current facial expression recognition methods based on deep learning are divided into two categories: Static facial expression recognition and dynamic facial expression recognition. The two methodswill be introduced and summarized respectively. Aiming at the advanced deep expression recognition algorithms in the ﬁeld, the performance of these algorithms on common expression databases is compared, and the strengths and weaknesses of these algorithms are analyzed in detail. Discussion and results: As the task of facial expression recognition is gradually transferred from the controlled laboratory environment to the challenging real-world environment, with the rapid development of deep learning technology, deep neural network can learn discriminative features, and is gradually applied to automatic facial expression recognition task. The current deep facial expression recognition system is committed to solve the following two problems: (1) Overﬁtting due to lack of suﬃcient training data; (2) In the real world environment, other variables that have nothing to do with expression bring interference problems.
Conclusion: From the perspective of algorithm, combining other expression models, such as facial action unit model and pleasure arousal dimension model, as well as other multimodal models, such as audio mode, 3D face depth information and human physiological information, can make expression recognition more practical.},
	language = {en},
	urldate = {2022-11-07},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Ge, Huilin and Zhu, Zhiyu and Dai, Yuewei and Wang, Biao and Wu, Xuedong},
	month = mar,
	year = {2022},
	pages = {106621},
	file = {Ge et al. - 2022 - Facial expression recognition based on deep learni.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\8Z7K8HLX\\Ge et al. - 2022 - Facial expression recognition based on deep learni.pdf:application/pdf},
}

@article{cid_novel_nodate,
	title = {A {Novel} {Multimodal} {Emotion} {Recognition} {Approach} for {Affective} {Human} {Robot} {Interaction}},
	abstract = {Facial expressions and speech are elements that provide emotional information about the user through multiple communication channels. In this paper, a novel multimodal emotion recognition system based on visual and auditory information processing is proposed. The proposed approach is used in real affective human robot communication in order to estimate ﬁve different emotional states (i.e., happiness, anger, fear, sadness and neutral), and it consists of two subsystems with similar structure. The ﬁrst subsystem achieves a robust facial feature extraction based on consecutively applied ﬁlters to the edge image and the use of a Dynamic Bayessian Classiﬁer. A similar classiﬁer is used in the second subsystem, where the input is associated to a set of speech descriptors, such as speech-rate, energy and pitch. Both subsystems are ﬁnally combined in real time. The results of this multimodal approach show the robustness and accuracy of the methodology respect to single emotion recognition systems.},
	language = {en},
	author = {Cid, Felipe and Manso, Luis J and Nunez, Pedro},
	pages = {9},
	file = {Cid et al. - A Novel Multimodal Emotion Recognition Approach fo.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\YGDPRW5F\\Cid et al. - A Novel Multimodal Emotion Recognition Approach fo.pdf:application/pdf},
}

@incollection{escalera_challenges_2017,
	address = {Cham},
	series = {The {Springer} {Series} on {Challenges} in {Machine} {Learning}},
	title = {Challenges in {Multi}-modal {Gesture} {Recognition}},
	isbn = {978-3-319-57021-1},
	url = {https://doi.org/10.1007/978-3-319-57021-1_1},
	abstract = {This paper surveys the state of the art on multimodal gesture recognition and introduces the JMLR special topic on gesture recognition 2011–2015. We began right at the start of the Kinect\$\${\textasciicircum}{\textbackslash}mathrm\{TM\}\$\$ revolution when inexpensive infrared cameras providing image depth recordings became available. We published papers using this technology and other more conventional methods, including regular video cameras, to record data, thus providing a good overview of uses of machine learning and computer vision using multimodal data in this area of application. Notably, we organized a series of challenges and made available several datasets we recorded for that purpose, including tens of thousands of videos, which are available to conduct further research. We also overview recent state of the art works on gesture recognition based on a proposed taxonomy for gesture recognition, discussing challenges and future lines of research.},
	language = {en},
	urldate = {2022-11-08},
	booktitle = {Gesture {Recognition}},
	publisher = {Springer International Publishing},
	author = {Escalera, Sergio and Athitsos, Vassilis and Guyon, Isabelle},
	editor = {Escalera, Sergio and Guyon, Isabelle and Athitsos, Vassilis},
	year = {2017},
	doi = {10.1007/978-3-319-57021-1_1},
	keywords = {Computer vision, Gesture recognition, Infrared cameras, Kinect
TM
TM, Multimodal data analysis, Pattern recognition, Time series analysis, Wearable sensors},
	pages = {1--60},
	file = {Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\VFDQDR3F\\Escalera et al. - 2017 - Challenges in Multi-modal Gesture Recognition.pdf:application/pdf},
}

@article{noroozi_survey_2021,
	title = {Survey on {Emotional} {Body} {Gesture} {Recognition}},
	volume = {12},
	issn = {1949-3045},
	doi = {10.1109/TAFFC.2018.2874986},
	abstract = {Automatic emotion recognition has become a trending research topic in the past decade. While works based on facial expressions or speech abound, recognizing affect from body gestures remains a less explored topic. We present a new comprehensive survey hoping to boost research in the field. We first introduce emotional body gestures as a component of what is commonly known as ”body language” and comment general aspects as gender differences and culture dependence. We then define a complete framework for automatic emotional body gesture recognition. We introduce person detection and comment static and dynamic body pose estimation methods both in RGB and 3D. We then comment the recent literature related to representation learning and emotion recognition from images of emotionally expressive gestures. We also discuss multi-modal approaches that combine speech or face with body gestures for improved emotion recognition. While pre-processing methodologies (e.g., human detection and pose estimation) are nowadays mature technologies fully developed for robust large scale analysis, we show that for emotion recognition the quantity of labelled data is scarce. There is no agreement on clearly defined output spaces and the representations are shallow and largely based on naive geometrical representations.},
	number = {2},
	journal = {IEEE Transactions on Affective Computing},
	author = {Noroozi, Fatemeh and Corneanu, Ciprian Adrian and Kamińska, Dorota and Sapiński, Tomasz and Escalera, Sergio and Anbarjafari, Gholamreza},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {affective computing, Emotion recognition, Affective computing, Face recognition, emotion recognition, Speech recognition, Gesture recognition, body pose estimation, emotional body gesture, Emotional body language, Legged locomotion, Pose estimation},
	pages = {505--523},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DFCS-General\\Zotero\\storage\\358A72TT\\8493586.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\GAE5JRKN\\Noroozi et al. - 2021 - Survey on Emotional Body Gesture Recognition.pdf:application/pdf},
}

@misc{noauthor_facial_nodate,
	title = {Facial {Action} {Coding} {System} - {PsycNET}},
	url = {https://psycnet.apa.org/doiLanding?doi=10.1037%2Ft27734-000},
	abstract = {APA PsycNet DoiLanding page},
	language = {en},
	urldate = {2022-11-08},
	file = {Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\Q8SZI3P4\\doiLanding.html:text/html},
}

@article{canal_survey_2022,
	title = {A survey on facial emotion recognition techniques: {A} state-of-the-art literature review},
	volume = {582},
	issn = {0020-0255},
	shorttitle = {A survey on facial emotion recognition techniques},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521010136},
	doi = {10.1016/j.ins.2021.10.005},
	abstract = {In this survey, a systematic literature review of the state-of-the-art on emotion expression recognition from facial images is presented. The paper has as main objective arise the most commonly used strategies employed to interpret and recognize facial emotion expressions, published over the past few years. For this purpose, a total of 51 papers were analyzed over the literature totaling 94 distinct methods, collected from well-established scientific databases (ACM Digital Library, IEEE Xplore, Science Direct and Scopus), whose works were categorized according to its main construction concept. From the analyzed works, it was possible to categorize them into two main trends: classical and those approaches specifically designed by the use of neural networks. The obtained statistical analysis demonstrated a marginally better recognition precision for the classical approaches when faced to neural networks counterpart, but with a reduced capacity of generalization. Additionally, the present study verified the most popular datasets for facial expression and emotion recognition showing the pros and cons each and, thereby, demonstrating a real demand for reliable data-sources regarding artificial and natural experimental environments.},
	language = {en},
	urldate = {2022-11-08},
	journal = {Information Sciences},
	author = {Canal, Felipe Zago and Müller, Tobias Rossi and Matias, Jhennifer Cristine and Scotton, Gustavo Gino and de Sa Junior, Antonio Reis and Pozzebon, Eliane and Sobieranski, Antonio Carlos},
	month = jan,
	year = {2022},
	keywords = {Systematic literature review, Pattern recognition, Emotion Recognition, Facial emotion recognition},
	pages = {593--617},
	file = {ScienceDirect Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\JA4QFM4Y\\S0020025521010136.html:text/html},
}

@article{savchenko_classifying_2022,
	title = {Classifying emotions and engagement in online learning based on a single facial expression recognition neural network},
	issn = {1949-3045},
	doi = {10.1109/TAFFC.2022.3188390},
	abstract = {In this paper, behaviour of students in the e-learning environment is analyzed. The novel pipeline is proposed based on video facial processing. At first, face detection, tracking and clustering techniques are applied to extract the sequences of faces of each student. Next, a single efficient neural network is used to extract emotional features in each frame. This network is pre-trained on face identification and fine-tuned for facial expression recognition on static images from AffectNet using a specially developed robust optimization technique. It is shown that the resulting facial features can be used for fast simultaneous prediction of students’ engagement levels (from disengaged to highly engaged), individual emotions (happy, sad, etc.,) and group-level affect (positive, neutral or negative). This model can be used for real-time video processing even on a mobile device of each student without the need for sending their facial video to the remote server or teacher’s PC. In addition, the possibility to prepare a summary of a lesson is demonstrated by saving short clips of different emotions and engagement of all students. The experimental study on the datasets from EmotiW (Emotion Recognition in the Wild) challenges showed that the proposed network significantly outperforms existing single models.},
	journal = {IEEE Transactions on Affective Computing},
	author = {Savchenko, Andrey V. and Savchenko, Lyudmila V. and Makarov, Ilya},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {Training, Emotion recognition, Face recognition, Feature extraction, Task analysis, Convolutional neural networks, e-learning, Electronic learning, engagement prediction, group-level emotion recognition, mobile devices, Online learning, video-based facial expression recognition},
	pages = {1--12},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DFCS-General\\Zotero\\storage\\RPWTQX3B\\9815154.html:text/html},
}

@article{li_adaptively_2021,
	title = {Adaptively {Learning} {Facial} {Expression} {Representation} via {C}-{F} {Labels} and {Distillation}},
	volume = {30},
	issn = {1941-0042},
	doi = {10.1109/TIP.2021.3049955},
	abstract = {Facial expression recognition is of significant importance in criminal investigation and digital entertainment. Under unconstrained conditions, existing expression datasets are highly class-imbalanced, and the similarity between expressions is high. Previous methods tend to improve the performance of facial expression recognition through deeper or wider network structures, resulting in increased storage and computing costs. In this paper, we propose a new adaptive supervised objective named AdaReg loss, re-weighting category importance coefficients to address this class imbalance and increasing the discrimination power of expression representations. Inspired by human beings’ cognitive mode, an innovative coarse-fine (C-F) labels strategy is designed to guide the model from easy to difficult to classify highly similar representations. On this basis, we propose a novel training framework named the emotional education mechanism (EEM) to transfer knowledge, composed of a knowledgeable teacher network (KTN) and a self-taught student network (STSN). Specifically, KTN integrates the outputs of coarse and fine streams, learning expression representations from easy to difficult. Under the supervision of the pre-trained KTN and existing learning experience, STSN can maximize the potential performance and compress the original KTN. Extensive experiments on public benchmarks demonstrate that the proposed method achieves superior performance compared to current state-of-the-art frameworks with 88.07\% on RAF-DB, 63.97\% on AffectNet and 90.49\% on FERPlus.},
	journal = {IEEE Transactions on Image Processing},
	author = {Li, Hangyu and Wang, Nannan and Ding, Xinpeng and Yang, Xi and Gao, Xinbo},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Training, Facial expression recognition, Face recognition, Feature extraction, Adaptation models, adaptive regular loss, coarse-fine labels, emotional education mechanism, Faces, Image coding, knowledge distillation, Mouth},
	pages = {2016--2028},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DFCS-General\\Zotero\\storage\\A4DGXR8B\\9321757.html:text/html},
}

@misc{aouayeb_learning_2021,
	title = {Learning {Vision} {Transformer} with {Squeeze} and {Excitation} for {Facial} {Expression} {Recognition}},
	url = {http://arxiv.org/abs/2107.03107},
	abstract = {As various databases of facial expressions have been made accessible over the last few decades, the Facial Expression Recognition (FER) task has gotten a lot of interest. The multiple sources of the available databases raised several challenges for facial recognition task. These challenges are usually addressed by Convolution Neural Network (CNN) architectures. Different from CNN models, a Transformer model based on attention mechanism has been presented recently to address vision tasks. One of the major issue with Transformers is the need of a large data for training, while most FER databases are limited compared to other vision applications. Therefore, we propose in this paper to learn a vision Transformer jointly with a Squeeze and Excitation (SE) block for FER task. The proposed method is evaluated on different publicly available FER databases including CK+, JAFFE,RAF-DB and SFEW. Experiments demonstrate that our model outperforms state-of-the-art methods on CK+ and SFEW and achieves competitive results on JAFFE and RAF-DB.},
	urldate = {2022-11-08},
	publisher = {arXiv},
	author = {Aouayeb, Mouath and Hamidouche, Wassim and Soladie, Catherine and Kpalma, Kidiyo and Seguier, Renaud},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03107 [cs]
version: 4},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\3JEIKM5I\\Aouayeb et al. - 2021 - Learning Vision Transformer with Squeeze and Excit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\PSCF5Y5F\\2107.html:text/html},
}

@article{plutchik_nature_2001,
	title = {The {Nature} of {Emotions}: {Human} emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice},
	volume = {89},
	issn = {0003-0996},
	shorttitle = {The {Nature} of {Emotions}},
	url = {https://www.jstor.org/stable/27857503},
	number = {4},
	urldate = {2022-11-09},
	journal = {American Scientist},
	author = {Plutchik, Robert},
	year = {2001},
	note = {Publisher: Sigma Xi, The Scientific Research Society},
	pages = {344--350},
}

@article{ekman_argument_1992,
	title = {An argument for basic emotions},
	volume = {6},
	issn = {0269-9931},
	url = {https://doi.org/10.1080/02699939208411068},
	doi = {10.1080/02699939208411068},
	abstract = {Emotions are viewed as having evolved through their adaptive value in dealing with fundamental life-tasks. Each emotion has unique features: signal, physiology, and antecedent events. Each emotion also has characteristics in common with other emotions: rapid onset, short duration, unbidden occurrence, automatic appraisal, and coherence among responses. These shared and unique characteristics are the product of our evolution, and distinguish emotions from other affective phenomena.},
	number = {3-4},
	urldate = {2022-11-09},
	journal = {Cognition and Emotion},
	author = {Ekman, Paul},
	month = may,
	year = {1992},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/02699939208411068},
	pages = {169--200},
}

@book{darwin_expression_1896,
	title = {The {Expression} of the {Emotions} in {Man} and {Animals}},
	isbn = {978-0-404-08410-3},
	abstract = {Previously published: London: J. Murray, 1890.},
	language = {en},
	publisher = {AMS Press},
	author = {Darwin, Charles},
	year = {1896},
	note = {Google-Books-ID: GytQXJiZmHEC},
	keywords = {Psychology / General},
}

@inproceedings{lian_context-dependent_2020,
	title = {Context-{Dependent} {Domain} {Adversarial} {Neural} {Network} for {Multimodal} {Emotion} {Recognition}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/lian20b_interspeech.html},
	doi = {10.21437/Interspeech.2020-1705},
	abstract = {Emotion recognition remains a complex task due to speaker variations and low-resource training samples. To address these difﬁculties, we focus on the domain adversarial neural networks (DANN) for emotion recognition. The primary task is to predict emotion labels. The secondary task is to learn a common representation where speaker identities can not be distinguished. By using this approach, we bring the representations of different speakers closer. Meanwhile, through using the unlabeled data in the training process, we alleviate the impact of lowresource training samples. In the meantime, prior work found that contextual information and multimodal features are important for emotion recognition. However, previous DANN based approaches ignore these information, thus limiting their performance. In this paper, we propose the context-dependent domain adversarial neural network for multimodal emotion recognition. To verify the effectiveness of our proposed method, we conduct experiments on the benchmark dataset IEMOCAP. Experimental results demonstrate that the proposed method shows an absolute improvement of 3.48\% over state-of-the-art strategies.},
	language = {en},
	urldate = {2022-11-09},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Lian, Zheng and Tao, Jianhua and Liu, Bin and Huang, Jian and Yang, Zhanlei and Li, Rongjun},
	month = oct,
	year = {2020},
	pages = {394--398},
	file = {Lian et al. - 2020 - Context-Dependent Domain Adversarial Neural Networ.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\5IQD6PE9\\Lian et al. - 2020 - Context-Dependent Domain Adversarial Neural Networ.pdf:application/pdf},
}

@article{wagner_dawn_nodate,
	title = {{DAWN} {OF} {THE} {TRANSFORMER} {ERA} {IN} {SPEECH} {EMOTION} {RECOGNITION}: {CLOSING} {THE} {VALENCE} {GAP}},
	abstract = {Recent advances in transformer-based architectures which are pre-trained in self-supervised manner have shown great promise in several machine learning tasks. In the audio domain, such architectures have also been successfully utilised in the ﬁeld of speech emotion recognition (SER). However, existing works have not evaluated the inﬂuence of model size and pre-training data on downstream performance, and have shown limited attention to generalisation, robustness, fairness, and efﬁciency. The present contribution conducts a thorough analysis of these aspects on several pre-trained variants of wav2vec 2.0 and HuBERT that we ﬁne-tuned on the dimensions arousal, dominance, and valence of MSP-Podcast, while additionally using IEMOCAP and MOSI to test cross-corpus generalisation. To the best of our knowledge, we obtain the top performance for valence prediction without use of explicit linguistic information, with a concordance correlation coefﬁcient (CCC) of .638 on MSP-Podcast. Furthermore, our investigations reveal that transformer-based architectures are more robust to small perturbations compared to a CNN-based baseline and fair with respect to biological sex groups, but not towards individual speakers. Finally, we are the ﬁrst to show that their extraordinary success on valence is based on implicit linguistic information learnt during ﬁnetuning of the transformer layers, which explains why they perform on-par with recent multimodal approaches that explicitly utilise textual information. Our ﬁndings collectively paint the following picture: transformer-based architectures constitute the new state-of-the-art in SER, but further advances are needed to mitigate remaining robustness and individual speaker issues. To make our ﬁndings reproducible, we release the best performing model to the community.},
	language = {en},
	author = {Wagner, Johannes and Triantafyllopoulos, Andreas and Wierstorf, Hagen and Schmitt, Maximilian and Burkhardt, Felix and Eyben, Florian and Schuller, Bjorn W},
	pages = {25},
	file = {Wagner et al. - DAWN OF THE TRANSFORMER ERA IN SPEECH EMOTION RECO.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\9FNVK9XM\\Wagner et al. - DAWN OF THE TRANSFORMER ERA IN SPEECH EMOTION RECO.pdf:application/pdf},
}

@article{abdulmohsin_new_2021,
	title = {A new proposed statistical feature extraction method in speech emotion recognition},
	volume = {93},
	issn = {00457906},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045790621001749},
	doi = {10.1016/j.compeleceng.2021.107172},
	abstract = {Feature extraction is the most important step in pattern recognition systems, and researchers have extensively focused on this field. This work aims to design and implement a novel feature extraction method that can extract features to recognize different emotions. Through this work, a unimodal speech, real-time, gender and speaker independent speech emotion recognition (SER) framework has been designed and implemented using the newly proposed extracted statistical features. This work’s contribution to feature extraction is the approach followed in extracting the statistical feature that used many degrees of the standard deviation (SD) on either side of the mean rather than 2 SDs on either side of the mean, as all researchers did in the past. In this work, the degrees of deviation on either side of the mean to study the feature distribution variance around the mean are (0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.5 and 4). The data sets used in this work were the Ryerson Audio-Visual Database of Emotional Speech and Song dataset (RAVDESS) with eight emotions, the Berlin dataset (Emo-DB) with seven emotions and the Surrey Audio-Visual Expressed Emotion dataset (SAVEE) with seven emotions. Compared to the state-of-the-art unimodal SER approaches, the classification accuracy achieved in this work was near perfect at 86.1\%, 96.3\% and 91.7\% for the RAVDESS, Emo-DB and SAVEE datasets, respectively.},
	language = {en},
	urldate = {2022-11-09},
	journal = {Computers \& Electrical Engineering},
	author = {Abdulmohsin, Husam Ali and Abdul wahab, Hala Bahjat and Abdul hossen, Abdul Mohssen Jaber},
	month = jul,
	year = {2021},
	pages = {107172},
	file = {Abdulmohsin et al. - 2021 - A new proposed statistical feature extraction meth.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\XRUJ6XH4\\Abdulmohsin et al. - 2021 - A new proposed statistical feature extraction meth.pdf:application/pdf},
}

@article{xiao_4d_2022,
	title = {{4D} attention-based neural network for {EEG} emotion recognition},
	volume = {16},
	issn = {1871-4080, 1871-4099},
	url = {https://link.springer.com/10.1007/s11571-021-09751-5},
	doi = {10.1007/s11571-021-09751-5},
	abstract = {Electroencephalograph (EEG) emotion recognition is a significant task in the brain-computer interface field. Although many deep learning methods are proposed recently, it is still challenging to make full use of the information contained in different domains of EEG signals. In this paper, we present a novel method, called four-dimensional attention-based neural network (4D-aNN) for EEG emotion recognition. First, raw EEG signals are transformed into 4D spatial-spectral-temporal representations. Then, the proposed 4D-aNN adopts spectral and spatial attention mechanisms to adaptively assign the weights of different brain regions and frequency bands, and a convolutional neural network (CNN) is utilized to deal with the spectral and spatial information of the 4D representations. Moreover, a temporal attention mechanism is integrated into a bidirectional Long Short-Term Memory (LSTM) to explore temporal dependencies of the 4D representations. Our model achieves state-of-the-art performance on the SEED dataset under intra-subject splitting. The experimental results have shown the effectiveness of the attention mechanisms in different domains for EEG emotion recognition.},
	language = {en},
	number = {4},
	urldate = {2022-11-09},
	journal = {Cognitive Neurodynamics},
	author = {Xiao, Guowen and Shi, Meng and Ye, Mengwen and Xu, Bowen and Chen, Zhendi and Ren, Quansheng},
	month = aug,
	year = {2022},
	pages = {805--818},
	file = {Xiao et al. - 2022 - 4D attention-based neural network for EEG emotion .pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\N7SHHZMJ\\Xiao et al. - 2022 - 4D attention-based neural network for EEG emotion .pdf:application/pdf},
}

@misc{koromilas_unsupervised_2022,
	title = {Unsupervised {Multimodal} {Language} {Representations} using {Convolutional} {Autoencoders}},
	url = {http://arxiv.org/abs/2110.03007},
	doi = {10.48550/arXiv.2110.03007},
	abstract = {Multimodal Language Analysis is a demanding area of research, since it is associated with two requirements: combining different modalities and capturing temporal information. During the last years, several works have been proposed in the area, mostly centered around supervised learning in downstream tasks. In this paper we propose extracting unsupervised Multimodal Language representations that are universal and can be applied to different tasks. Towards this end, we map the word-level aligned multimodal sequences to 2-D matrices and then use Convolutional Autoencoders to learn embeddings by combining multiple datasets. Extensive experimentation on Sentiment Analysis (MOSEI) and Emotion Recognition (IEMOCAP) indicate that the learned representations can achieve near-state-of-the-art performance with just the use of a Logistic Regression algorithm for downstream classification. It is also shown that our method is extremely lightweight and can be easily generalized to other tasks and unseen data with small performance drop and almost the same number of parameters. The proposed multimodal representation models are open-sourced and will help grow the applicability of Multimodal Language.},
	urldate = {2022-11-09},
	publisher = {arXiv},
	author = {Koromilas, Panagiotis and Giannakopoulos, Theodoros},
	month = jan,
	year = {2022},
	note = {arXiv:2110.03007 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Computation and Language},
	annote = {Comment: 5 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\8JQWGFBX\\Koromilas and Giannakopoulos - 2022 - Unsupervised Multimodal Language Representations u.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\7U746I8E\\2110.html:text/html},
}

@inproceedings{bagher_zadeh_multimodal_2018,
	address = {Melbourne, Australia},
	title = {Multimodal {Language} {Analysis} in the {Wild}: {CMU}-{MOSEI} {Dataset} and {Interpretable} {Dynamic} {Fusion} {Graph}},
	shorttitle = {Multimodal {Language} {Analysis} in the {Wild}},
	url = {https://aclanthology.org/P18-1208},
	doi = {10.18653/v1/P18-1208},
	abstract = {Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.},
	urldate = {2022-11-09},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Bagher Zadeh, AmirAli and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
	month = jul,
	year = {2018},
	pages = {2236--2246},
	file = {Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\6MV84SX8\\Bagher Zadeh et al. - 2018 - Multimodal Language Analysis in the Wild CMU-MOSE.pdf:application/pdf},
}

@misc{paraskevopoulos_mmlatch_2022,
	title = {{MMLatch}: {Bottom}-up {Top}-down {Fusion} for {Multimodal} {Sentiment} {Analysis}},
	shorttitle = {{MMLatch}},
	url = {http://arxiv.org/abs/2201.09828},
	abstract = {Current deep learning approaches for multimodal fusion rely on bottom-up fusion of high and mid-level latent modality representations (late/mid fusion) or low level sensory inputs (early fusion). Models of human perception highlight the importance of top-down fusion, where high-level representations affect the way sensory inputs are perceived, i.e. cognition affects perception. These top-down interactions are not captured in current deep learning models. In this work we propose a neural architecture that captures top-down cross-modal interactions, using a feedback mechanism in the forward pass during network training. The proposed mechanism extracts high-level representations for each modality and uses these representations to mask the sensory inputs, allowing the model to perform top-down feature masking. We apply the proposed model for multimodal sentiment recognition on CMU-MOSEI. Our method shows consistent improvements over the well established MulT and over our strong late fusion baseline, achieving state-of-the-art results.},
	urldate = {2022-11-09},
	publisher = {arXiv},
	author = {Paraskevopoulos, Georgios and Georgiou, Efthymios and Potamianos, Alexandros},
	month = jan,
	year = {2022},
	note = {arXiv:2201.09828 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Accepted, ICASSP 2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\4X75TIDV\\Paraskevopoulos et al. - 2022 - MMLatch Bottom-up Top-down Fusion for Multimodal .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\KJLNZDY3\\2201.html:text/html},
}

@misc{stappen_multimodal_2021,
	title = {The {Multimodal} {Sentiment} {Analysis} in {Car} {Reviews} ({MuSe}-{CaR}) {Dataset}: {Collection}, {Insights} and {Improvements}},
	shorttitle = {The {Multimodal} {Sentiment} {Analysis} in {Car} {Reviews} ({MuSe}-{CaR}) {Dataset}},
	url = {http://arxiv.org/abs/2101.06053},
	abstract = {Truly real-life data presents a strong, but exciting challenge for sentiment and emotion research. The high variety of possible `in-the-wild' properties makes large datasets such as these indispensable with respect to building robust machine learning models. A sufficient quantity of data covering a deep variety in the challenges of each modality to force the exploratory analysis of the interplay of all modalities has not yet been made available in this context. In this contribution, we present MuSe-CaR, a first of its kind multimodal dataset. The data is publicly available as it recently served as the testing bed for the 1st Multimodal Sentiment Analysis Challenge, and focused on the tasks of emotion, emotion-target engagement, and trustworthiness recognition by means of comprehensively integrating the audio-visual and language modalities. Furthermore, we give a thorough overview of the dataset in terms of collection and annotation, including annotation tiers not used in this year's MuSe 2020. In addition, for one of the sub-challenges - predicting the level of trustworthiness - no participant outperformed the baseline model, and so we propose a simple, but highly efficient Multi-Head-Attention network that exceeds using multimodal fusion the baseline by around 0.2 CCC (almost 50 \% improvement).},
	urldate = {2022-11-09},
	publisher = {arXiv},
	author = {Stappen, Lukas and Baird, Alice and Schumann, Lea and Schuller, Björn},
	month = oct,
	year = {2021},
	note = {arXiv:2101.06053 [cs]},
	keywords = {Computer Science - Multimedia, Computer Science - Computation and Language},
	annote = {Comment: accepted version},
	file = {arXiv Fulltext PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\A2CA75A3\\Stappen et al. - 2021 - The Multimodal Sentiment Analysis in Car Reviews (.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\YNT45JVB\\2101.html:text/html},
}

@misc{joshi_cogmen_2022,
	title = {{COGMEN}: {COntextualized} {GNN} based {Multimodal} {Emotion} {recognitioN}},
	shorttitle = {{COGMEN}},
	url = {http://arxiv.org/abs/2205.02455},
	abstract = {Emotions are an inherent part of human interactions, and consequently, it is imperative to develop AI systems that understand and recognize human emotions. During a conversation involving various people, a person's emotions are influenced by the other speaker's utterances and their own emotional state over the utterances. In this paper, we propose COntextualized Graph Neural Network based Multimodal Emotion recognitioN (COGMEN) system that leverages local information (i.e., inter/intra dependency between speakers) and global information (context). The proposed model uses Graph Neural Network (GNN) based architecture to model the complex dependencies (local and global information) in a conversation. Our model gives state-of-the-art (SOTA) results on IEMOCAP and MOSEI datasets, and detailed ablation experiments show the importance of modeling information at both levels.},
	urldate = {2022-11-09},
	publisher = {arXiv},
	author = {Joshi, Abhinav and Bhat, Ashwani and Jain, Ayush and Singh, Atin Vikram and Modi, Ashutosh},
	month = may,
	year = {2022},
	note = {arXiv:2205.02455 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 17 pages (9 main + 8 appendix). Accepted at NAACL 2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\5TZZ5ABK\\Joshi et al. - 2022 - COGMEN COntextualized GNN based Multimodal Emotio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\7QCVUXN2\\2205.html:text/html},
}

@misc{atul_ai_2018,
	title = {{AI} vs {Machine} {Learning} vs {Deep} {Learning}},
	url = {https://www.edureka.co/blog/ai-vs-machine-learning-vs-deep-learning/},
	abstract = {AI vs Machine Learning vs Deep Learning - Artificial Intelligence is the broader umbrella under which Machine Learning and Deep Learning come. And deep learning is a subset of Machine Learning. So all three of them AI, machine learning and deep learning are just the subsets of each other.},
	language = {en-US},
	urldate = {2022-12-05},
	journal = {Edureka},
	author = {Atul},
	month = jun,
	year = {2018},
	note = {Section: Data Science},
	file = {Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\UCEWXQMN\\ai-vs-machine-learning-vs-deep-learning.html:text/html},
}

@article{mollahosseini_affectnet_2019,
	title = {{AffectNet}: {A} {Database} for {Facial} {Expression}, {Valence}, and {Arousal} {Computing} in the {Wild}},
	volume = {10},
	issn = {1949-3045},
	shorttitle = {{AffectNet}},
	doi = {10.1109/TAFFC.2017.2740923},
	abstract = {Automated affective computing in the wild setting is a challenging problem in computer vision. Existing annotated databases of facial expressions in the wild are small and mostly cover discrete emotions (aka the categorical model). There are very limited annotated facial databases for affective computing in the continuous dimensional model (e.g., valence and arousal). To meet this need, we collected, annotated, and prepared for public distribution a new database of facial emotions in the wild (called AffectNet). AffectNet contains more than 1,000,000 facial images from the Internet by querying three major search engines using 1,250 emotion related keywords in six different languages. About half of the retrieved images were manually annotated for the presence of seven discrete facial expressions and the intensity of valence and arousal. AffectNet is by far the largest database of facial expression, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. Two baseline deep neural networks are used to classify images in the categorical model and predict the intensity of valence and arousal. Various evaluation metrics show that our deep neural network baselines can perform better than conventional machine learning methods and off-the-shelf facial expression recognition systems.},
	number = {1},
	journal = {IEEE Transactions on Affective Computing},
	author = {Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad H.},
	month = jan,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {Affective computing, Affective computing in the wild, arousal, Computational modeling, continuous dimensional space, Databases, Face, Face recognition, facial expressions, Magnetic heads, valence},
	pages = {18--31},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DFCS-General\\Zotero\\storage\\WZER3U7F\\8013713.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\2HR9TYPP\\Mollahosseini et al. - 2019 - AffectNet A Database for Facial Expression, Valen.pdf:application/pdf},
}

@inproceedings{antoniadis_exploiting_2021,
	title = {Exploiting {Emotional} {Dependencies} with {Graph} {Convolutional} {Networks} for {Facial} {Expression} {Recognition}},
	url = {http://arxiv.org/abs/2106.03487},
	doi = {10.1109/FG52635.2021.9667014},
	abstract = {Over the past few years, deep learning methods have shown remarkable results in many face-related tasks including automatic facial expression recognition (FER) in-the-wild. Meanwhile, numerous models describing the human emotional states have been proposed by the psychology community. However, we have no clear evidence as to which representation is more appropriate and the majority of FER systems use either the categorical or the dimensional model of affect. Inspired by recent work in multi-label classification, this paper proposes a novel multi-task learning (MTL) framework that exploits the dependencies between these two models using a Graph Convolutional Network (GCN) to recognize facial expressions in-the-wild. Specifically, a shared feature representation is learned for both discrete and continuous recognition in a MTL setting. Moreover, the facial expression classifiers and the valence-arousal regressors are learned through a GCN that explicitly captures the dependencies between them. To evaluate the performance of our method under real-world conditions we perform extensive experiments on the AffectNet and Aff-Wild2 datasets. The results of our experiments show that our method is capable of improving the performance across different datasets and backbone architectures. Finally, we also surpass the previous state-of-the-art methods on the categorical model of AffectNet.},
	urldate = {2022-12-05},
	booktitle = {2021 16th {IEEE} {International} {Conference} on {Automatic} {Face} and {Gesture} {Recognition} ({FG} 2021)},
	author = {Antoniadis, Panagiotis and Filntisis, Panagiotis P. and Maragos, Petros},
	month = dec,
	year = {2021},
	note = {arXiv:2106.03487 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1--8},
	annote = {Comment: 9 pages, 8 figures, 5 tables, revised submission to the 16th IEEE International Conference on Automatic Face and Gesture Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\7QJ5RUFF\\Antoniadis et al. - 2021 - Exploiting Emotional Dependencies with Graph Convo.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\DZMDEXP7\\2106.html:text/html},
}

@inproceedings{raju_continuous_2021,
	title = {Continuous {Multi}-modal {Emotion} {Prediction} in {Video} based on {Recurrent} {Neural} {Network} {Variants} with {Attention}},
	doi = {10.1109/ICMLA52953.2021.00115},
	abstract = {Automatic perception and understanding of human emotion is becoming an increasingly attractive research field in artificial intelligence and human-computer interaction. Emotion portrayal within conversation plays a significant role in the semantics of a sentence. However, emotion is not only biologically determined but is also influenced by the environment. Therefore, cultural differences exist in some aspects of emotions, and it is important for the next generation of computer systems to adapt the cross-cultural difference in order to enable more naturalistic interactions between humans and machines. In this paper, we investigate the suitability of state-of-the-art deep learning architectures based on recurrent neural network (RNN) variants with explicit attention modelling to bridge the gap across different cultures (German and Hungarian) for emotion prediction in video. Three different attention based network architectures are proposed in this work:- early attention fusion, extended multi-attention fusion and attention-based encoder-decoder. Our RNN variants with explicit attention modelling approach achieves very promising Concordance Correlation Coefficient results, which outperform the baseline on Arousal of 0.637 vs. 0.614 (baseline), for Valence of 0.689 vs. 0.615 and for Liking of 0.625 vs. 0.222.},
	booktitle = {2021 20th {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Raju, Joyal and Gaus, Yona Falinie A. and Breckon, Toby P.},
	month = dec,
	year = {2021},
	keywords = {affective computing, attention network, Computer architecture, cross-cultural, Deep learning, emotion, emotion recognition, Human computer interaction, motion detection, multi-modal, Predictive models, Recurrent neural networks, Semantics, Text recognition},
	pages = {688--693},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DFCS-General\\Zotero\\storage\\XP5KKW8E\\9680001.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\SKPQVFIJ\\Raju et al. - 2021 - Continuous Multi-modal Emotion Prediction in Video.pdf:application/pdf},
}

@inproceedings{zhang_biovid_2016,
	title = {“{BioVid} {Emo} {DB}”: {A} multimodal database for emotion analyses validated by subjective ratings},
	shorttitle = {“{BioVid} {Emo} {DB}”},
	doi = {10.1109/SSCI.2016.7849931},
	abstract = {The precondition of productive data mining is having an efficient database to work on. The BioVid Emo DB is a multimodal database recorded for the purpose of analyzing human affective states and data mining related to emotion. Psychophysiological signals such as Skin Conductance Level, Electrocardiogram, Trapezius Electromyogram and also 4 video signals were recorded. 5 discrete emotions (amusement, sadness, anger, disgust and fear) were elicited by 15 standardized film clips. 94 participants watched them, rated them in terms of the experienced emotional level and selected the film clips that evoked the strongest emotion. A preliminary analysis of the subjective ratings made during the experiment is presented. The dataset is available for other researchers.},
	booktitle = {2016 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	author = {Zhang, Lin and Walter, Steffen and Ma, Xueyao and Werner, Philipp and Al-Hamadi, Ayoub and Traue, Harald C. and Gruss, Sascha},
	month = dec,
	year = {2016},
	keywords = {Cameras, data mining, Data mining, database, Databases, discrete emotions, Electrodes, emotion recognition, film clips, Films, Skin, Streaming media},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DFCS-General\\Zotero\\storage\\B8STCYJQ\\7849931.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\ZYU3I8PC\\Zhang et al. - 2016 - “BioVid Emo DB” A multimodal database for emotion.pdf:application/pdf},
}

@misc{noauthor_emo-db_nodate,
	title = {Emo-{DB}},
	url = {http://emodb.bilderbar.info/docu/},
	urldate = {2022-12-05},
	file = {Emo-DB:C\:\\Users\\DFCS-General\\Zotero\\storage\\4MIZQGK3\\docu.html:text/html},
}

@article{busso_iemocap_2008,
	title = {{IEMOCAP}: interactive emotional dyadic motion capture database},
	volume = {42},
	issn = {1574-020X, 1574-0218},
	shorttitle = {{IEMOCAP}},
	url = {http://link.springer.com/10.1007/s10579-008-9076-6},
	doi = {10.1007/s10579-008-9076-6},
	abstract = {Since emotions are expressed through a combination of verbal and nonverbal channels, a joint analysis of speech and gestures is required to understand expressive human communication. To facilitate such investigations, this paper describes a new corpus named the ‘‘interactive emotional dyadic motion capture database’’ (IEMOCAP), collected by the Speech Analysis and Interpretation Laboratory (SAIL) at the University of Southern California (USC). This database was recorded from ten actors in dyadic sessions with markers on the face, head, and hands, which provide detailed information about their facial expressions and hand movements during scripted and spontaneous spoken communication scenarios. The actors performed selected emotional scripts and also improvised hypothetical scenarios designed to elicit speciﬁc types of emotions (happiness, anger, sadness, frustration and neutral state). The corpus contains approximately 12 h of data. The detailed motion capture information, the interactive setting to elicit authentic emotions, and the size of the database make this corpus a valuable addition to the existing databases in the community for the study and modeling of multimodal and expressive human communication.},
	language = {en},
	number = {4},
	urldate = {2022-12-05},
	journal = {Language Resources and Evaluation},
	author = {Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N. and Lee, Sungbok and Narayanan, Shrikanth S.},
	month = dec,
	year = {2008},
	pages = {335--359},
	file = {Busso et al. - 2008 - IEMOCAP interactive emotional dyadic motion captu.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\7KBBNQD5\\Busso et al. - 2008 - IEMOCAP interactive emotional dyadic motion captu.pdf:application/pdf},
}

@misc{ye_temporal_2022,
	title = {Temporal {Modeling} {Matters}: {A} {Novel} {Temporal} {Emotional} {Modeling} {Approach} for {Speech} {Emotion} {Recognition}},
	shorttitle = {Temporal {Modeling} {Matters}},
	url = {http://arxiv.org/abs/2211.08233},
	abstract = {Speech emotion recognition (SER) plays a vital role in improving the interactions between humans and machines by inferring human emotion and affective states from speech signals. Whereas recent works primarily focus on mining spatiotemporal information from hand-crafted features, we explore how to model the temporal patterns of speech emotions from dynamic temporal scales. Towards that goal, we introduce a novel temporal emotional modeling approach for SER, termed Temporal-aware bI-direction Multi-scale Network (TIM-Net), which learns multi-scale contextual affective representations from various time scales. Specifically, TIM-Net first employs temporal-aware blocks to learn temporal affective representation, then integrates complementary information from the past and the future to enrich contextual representations, and finally, fuses multiple time scale features for better adaptation to the emotional variation. Extensive experimental results on six benchmark SER datasets demonstrate the superior performance of TIM-Net, gaining 2.34\% and 2.61\% improvements of the average UAR and WAR over the second-best on each corpus. Remarkably, TIM-Net outperforms the latest domain-adaptation method on the cross-corpus SER tasks, demonstrating strong generalizability.},
	urldate = {2022-12-05},
	publisher = {arXiv},
	author = {Ye, Jiaxin and Wen, Xincheng and Wei, Yujie and Xu, Yong and Liu, Kunhong and Shan, Hongming},
	month = nov,
	year = {2022},
	note = {arXiv:2211.08233 [cs, eess]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Submitted to ICASSP 2023. 8 pages, 6 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\M42JCKUX\\Ye et al. - 2022 - Temporal Modeling Matters A Novel Temporal Emotio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\ZZRR3VK8\\2211.html:text/html},
}

@article{livingstone_ryerson_2018,
	title = {The {Ryerson} {Audio}-{Visual} {Database} of {Emotional} {Speech} and {Song} ({RAVDESS}): {A} dynamic, multimodal set of facial and vocal expressions in {North} {American} {English}},
	volume = {13},
	issn = {1932-6203},
	shorttitle = {The {Ryerson} {Audio}-{Visual} {Database} of {Emotional} {Speech} and {Song} ({RAVDESS})},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0196391},
	doi = {10.1371/journal.pone.0196391},
	abstract = {The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite "goodness" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.},
	language = {en},
	number = {5},
	urldate = {2022-12-05},
	journal = {PLOS ONE},
	author = {Livingstone, Steven R. and Russo, Frank A.},
	month = may,
	year = {2018},
	note = {Publisher: Public Library of Science},
	keywords = {Emotions, Face, Facial expressions, Fear, Musculoskeletal mechanics, Music cognition, Speech, Vocalization},
	pages = {e0196391},
	file = {Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\YVWMLSZQ\\Livingstone and Russo - 2018 - The Ryerson Audio-Visual Database of Emotional Spe.pdf:application/pdf;Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\FYJZEIGA\\article.html:text/html},
}

@misc{li_emocaps_2022,
	title = {{EmoCaps}: {Emotion} {Capsule} based {Model} for {Conversational} {Emotion} {Recognition}},
	shorttitle = {{EmoCaps}},
	url = {http://arxiv.org/abs/2203.13504},
	abstract = {Emotion recognition in conversation (ERC) aims to analyze the speaker's state and identify their emotion in the conversation. Recent works in ERC focus on context modeling but ignore the representation of contextual emotional tendency. In order to extract multi-modal information and the emotional tendency of the utterance effectively, we propose a new structure named Emoformer to extract multi-modal emotion vectors from different modalities and fuse them with sentence vector to be an emotion capsule. Furthermore, we design an end-to-end ERC model called EmoCaps, which extracts emotion vectors through the Emoformer structure and obtain the emotion classification results from a context analysis model. Through the experiments with two benchmark datasets, our model shows better performance than the existing state-of-the-art models.},
	urldate = {2022-12-05},
	publisher = {arXiv},
	author = {Li, Zaijing and Tang, Fengxiao and Zhao, Ming and Zhu, Yusen},
	month = mar,
	year = {2022},
	note = {arXiv:2203.13504 [cs, eess]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: 9 pages, 5 figures, accepted by Finding of ACL 2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\F5PYC5QS\\Li et al. - 2022 - EmoCaps Emotion Capsule based Model for Conversat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\E73LT28Y\\2203.html:text/html},
}

@inproceedings{martinez-lucas_msp-conversation_2020,
	title = {The {MSP}-{Conversation} {Corpus}},
	url = {https://www.isca-speech.org/archive/interspeech_2020/martinezlucas20_interspeech.html},
	doi = {10.21437/Interspeech.2020-2444},
	abstract = {Human-computer interactions can be very effective, especially if computers can automatically recognize the emotional state of the user. A key barrier for effective speech emotion recognition systems is the lack of large corpora annotated with emotional labels that reﬂect the temporal complexity of expressive behaviors, especially during multiparty interactions. This paper introduces the MSP-Conversation corpus, which contains interactions annotated with time-continuous emotional traces for arousal (calm to active), valence (negative to positive), and dominance (weak to strong). Time-continuous annotations offer the ﬂexibility to explore emotional displays at different temporal resolutions while leveraging contextual information. This is an ongoing effort, where the corpus currently contains more than 15 hours of speech annotated by at least ﬁve annotators. The data is sourced from the MSP-Podcast corpus, which contains speech data from online audio-sharing websites annotated with sentence-level emotional scores. This data collection scheme is an easy, affordable, and scalable approach to obtain natural data with diverse emotional content from multiple speakers. This study describes the key features of the corpus. It also compares the time-continuous evaluations from the MSPConversation corpus with the sentence-level annotations of the MSP-Podcast corpus for the speech segments that overlap between the two corpora.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Martinez-Lucas, Luz and Abdelwahab, Mohammed and Busso, Carlos},
	month = oct,
	year = {2020},
	pages = {1823--1827},
	file = {Martinez-Lucas et al. - 2020 - The MSP-Conversation Corpus.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\KUFZMH5A\\Martinez-Lucas et al. - 2020 - The MSP-Conversation Corpus.pdf:application/pdf},
}

@misc{jackson_surrey_nodate,
	title = {Surrey {Audio}-{Visual} {Expressed} {Emotion} ({SAVEE}) {Database}},
	url = {http://kahlan.eps.surrey.ac.uk/savee/},
	urldate = {2022-12-05},
	author = {Jackson, Philip and Haq, Sanaul},
	file = {Surrey Audio-Visual Expressed Emotion (SAVEE) Database:C\:\\Users\\DFCS-General\\Zotero\\storage\\CB64NW23\\savee.html:text/html},
}

@misc{noauthor_emovo_nodate,
	title = {Emovo {Corpus}: an {Italian} {Emotional} {Speech} {Database}},
	url = {https://art.torvergata.it/handle/2108/97109},
	urldate = {2022-12-05},
	file = {Emovo Corpus an Italian Emotional Speech Database.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\FI8Q5ZAV\\Emovo Corpus an Italian Emotional Speech Database.pdf:application/pdf;Emovo Corpus\: an Italian Emotional Speech Database:C\:\\Users\\DFCS-General\\Zotero\\storage\\FJR9FQR8\\97109.html:text/html},
}

@inproceedings{zhou_exploring_2019,
	address = {Suzhou China},
	title = {Exploring {Emotion} {Features} and {Fusion} {Strategies} for {Audio}-{Video} {Emotion} {Recognition}},
	isbn = {978-1-4503-6860-5},
	url = {https://dl.acm.org/doi/10.1145/3340555.3355713},
	doi = {10.1145/3340555.3355713},
	abstract = {The audio-video based emotion recognition aims to classify a given video into basic emotions. In this paper, we describe our approaches in EmotiW 2019, which mainly explores emotion features and feature fusion strategies for audio and visual modality. For emotion features, we explore audio feature with both speech-spectrogram and Log Mel-spectrogram and evaluate several facial features with different CNN models and different emotion pretrained strategies. For fusion strategies, we explore intra-modal and cross-modal fusion methods, such as designing attention mechanisms to highlights important emotion feature, exploring feature concatenation and factorized bilinear pooling (FBP) for cross-modal feature fusion. With careful evaluation, we obtain 65.5\% on the AFEW validation set and 62.48\% on the test set and rank third in the challenge.},
	language = {en},
	urldate = {2022-12-05},
	booktitle = {2019 {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {ACM},
	author = {Zhou, Hengshun and Meng, Debin and Zhang, Yuanyuan and Peng, Xiaojiang and Du, Jun and Wang, Kai and Qiao, Yu},
	month = oct,
	year = {2019},
	pages = {562--566},
	file = {Zhou et al. - 2019 - Exploring Emotion Features and Fusion Strategies f.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\MPTDMR3H\\Zhou et al. - 2019 - Exploring Emotion Features and Fusion Strategies f.pdf:application/pdf},
}

@article{dhall_acted_nodate,
	title = {Acted {Facial} {Expressions} {In} {The} {Wild} {Database}},
	abstract = {Quality data recorded in varied realistic environments is vital for effective human face related research. Currently available datasets for human facial expression analysis have been generated in highly controlled lab environments. We present a new dynamic 2D facial expressions database based on movies capturing diverse scenarios. A new XML schema based approach has been developed for the database collection and distribution tools. Our database captures varied facial expressions, natural head pose movements, occlusions, subjects from various races, gender, diverse ages and multiple subjects in a scene. The subjects have been labelled with information such as name, age of character, age of actor, gender, pose and individual facial expressions. In total, we used thirty-seven movie DVDs. The database consists of a detailed XML schema which contains information about the clips and experiment protocols.},
	language = {en},
	author = {Dhall, Abhinav and Goecke, Roland and Lucey, Simon and Gedeon, Tom},
	pages = {15},
	file = {Dhall et al. - Acted Facial Expressions In The Wild Database.pdf:C\:\\Users\\DFCS-General\\Zotero\\storage\\MKUNVLQQ\\Dhall et al. - Acted Facial Expressions In The Wild Database.pdf:application/pdf},
}

@inproceedings{lucey_extended_2010,
	title = {The {Extended} {Cohn}-{Kanade} {Dataset} ({CK}+): {A} complete dataset for action unit and emotion-specified expression},
	shorttitle = {The {Extended} {Cohn}-{Kanade} {Dataset} ({CK}+)},
	doi = {10.1109/CVPRW.2010.5543262},
	abstract = {In 2000, the Cohn-Kanade (CK) database was released for the purpose of promoting research into automatically detecting individual facial expressions. Since then, the CK database has become one of the most widely used test-beds for algorithm development and evaluation. During this period, three limitations have become apparent: 1) While AU codes are well validated, emotion labels are not, as they refer to what was requested rather than what was actually performed, 2) The lack of a common performance metric against which to evaluate new algorithms, and 3) Standard protocols for common databases have not emerged. As a consequence, the CK database has been used for both AU and emotion detection (even though labels for the latter have not been validated), comparison with benchmark algorithms is missing, and use of random subsets of the original database makes meta-analyses difficult. To address these and other concerns, we present the Extended Cohn-Kanade (CK+) database. The number of sequences is increased by 22\% and the number of subjects by 27\%. The target expression for each sequence is fully FACS coded and emotion labels have been revised and validated. In addition to this, non-posed sequences for several types of smiles and their associated metadata have been added. We present baseline results using Active Appearance Models (AAMs) and a linear support vector machine (SVM) classifier using a leave-one-out subject cross-validation for both AU and emotion detection for the posed data. The emotion and AU labels, along with the extended image data and tracked landmarks will be made available July 2010.},
	booktitle = {2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} - {Workshops}},
	author = {Lucey, Patrick and Cohn, Jeffrey F. and Kanade, Takeo and Saragih, Jason and Ambadar, Zara and Matthews, Iain},
	month = jun,
	year = {2010},
	note = {ISSN: 2160-7516},
	keywords = {Active appearance model, Code standards, Databases, Face detection, Gold, Measurement, Performance evaluation, Support vector machine classification, Support vector machines, Testing},
	pages = {94--101},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DFCS-General\\Zotero\\storage\\PIML6QB5\\5543262.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\X9E96TAN\\Lucey et al. - 2010 - The Extended Cohn-Kanade Dataset (CK+) A complete.pdf:application/pdf},
}

@misc{lyons_japanese_1998,
	title = {The {Japanese} {Female} {Facial} {Expression} ({JAFFE}) {Dataset}},
	url = {https://zenodo.org/record/3451524},
	abstract = {IMPORTANT: Before requesting access, READ and FOLLOW all of the instructions, or your request will certainly be rejected. Specifications: 10 Japanese female expressers 7 Posed Facial Expressions (6 basic facial expressions + 1 neutral) Several images of each expression for each expresser  213 images total Each image has averaged semantic ratings on 6 facial expressions by 60 Japanese viewers Resolution 256x256 pixels 8-bit grayscale Tiff format, no compression Documentation: README\_FIRST.txt and the articles linked below The image dataset was planned and assembled by Michael Lyons, Miyuki Kamachi, and Jiro Gyoba, at Kyushu University, Japan. The JAFFE images may be used for non-commercial scientific research under certain terms of use, which must be accepted to access the data. The following article describes the JAFFE images and how they were photographed: Michael J. Lyons, Miyuki Kamachi, Jiro Gyoba. Coding Facial Expressions with Gabor Wavelets (IVC Special Issue)  10.5281/zenodo.4029679 The following article explains the origin of JAFFE: Michael J. Lyons "Excavating AI" Re-excavated: Debunking a Fallacious Account of the JAFFE Dataset  10.5281/zenodo.5140556},
	language = {eng},
	urldate = {2022-12-05},
	publisher = {Zenodo},
	author = {Lyons, Michael and Kamachi, Miyuki and Gyoba, Jiro},
	month = apr,
	year = {1998},
	doi = {10.5281/zenodo.3451524},
	note = {Type: dataset},
	keywords = {affect, emotion, facial expression cognition, facial expression images, facial expression recognition, image datasets},
	annote = {The images are provided at no cost for non-commercial scientific research only. If you agree to the conditions listed below, you may request access to download.},
	file = {Zenodo Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\SHAUKVBG\\3451524.html:text/html},
}

@misc{zhang_learn_2022,
	title = {Learn {From} {All}: {Erasing} {Attention} {Consistency} for {Noisy} {Label} {Facial} {Expression} {Recognition}},
	shorttitle = {Learn {From} {All}},
	url = {http://arxiv.org/abs/2207.10299},
	abstract = {Noisy label Facial Expression Recognition (FER) is more challenging than traditional noisy label classification tasks due to the inter-class similarity and the annotation ambiguity. Recent works mainly tackle this problem by filtering out large-loss samples. In this paper, we explore dealing with noisy labels from a new feature-learning perspective. We find that FER models remember noisy samples by focusing on a part of the features that can be considered related to the noisy labels instead of learning from the whole features that lead to the latent truth. Inspired by that, we propose a novel Erasing Attention Consistency (EAC) method to suppress the noisy samples during the training process automatically. Specifically, we first utilize the flip semantic consistency of facial images to design an imbalanced framework. We then randomly erase input images and use flip attention consistency to prevent the model from focusing on a part of the features. EAC significantly outperforms state-of-the-art noisy label FER methods and generalizes well to other tasks with a large number of classes like CIFAR100 and Tiny-ImageNet. The code is available at https://github.com/zyh-uaiaaaa/Erasing-Attention-Consistency.},
	urldate = {2022-12-05},
	publisher = {arXiv},
	author = {Zhang, Yuhang and Wang, Chengrui and Ling, Xu and Deng, Weihong},
	month = sep,
	year = {2022},
	note = {arXiv:2207.10299 [cs]
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ECCV2022},
	file = {arXiv Fulltext PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\DWU8U5WV\\Zhang et al. - 2022 - Learn From All Erasing Attention Consistency for .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\MV32KNW9\\2207.html:text/html},
}

@article{miranda-correa_amigos_2021,
	title = {{AMIGOS}: {A} {Dataset} for {Affect}, {Personality} and {Mood} {Research} on {Individuals} and {Groups}},
	volume = {12},
	issn = {1949-3045},
	shorttitle = {{AMIGOS}},
	doi = {10.1109/TAFFC.2018.2884461},
	abstract = {We present AMIGOS- A dataset for Multimodal research of affect, personality traits and mood on Individuals and GrOupS. Different to other databases, we elicited affect using both short and long videos in two social contexts, one with individual viewers and one with groups of viewers. The database allows the multimodal study of the affective responses, by means of neuro-physiological signals of individuals in relation to their personality and mood, and with respect to the social context and videos' duration. The data is collected in two experimental settings. In the first one, 40 participants watched 16 short emotional videos. In the second one, the participants watched 4 long videos, some of them alone and the rest in groups. The participants' signals, namely, Electroencephalogram (EEG), Electrocardiogram (ECG) and Galvanic Skin Response (GSR), were recorded using wearable sensors. Participants' frontal HD video and both RGB and depth full body videos were also recorded. Participants emotions have been annotated with both self-assessment of affective levels (valence, arousal, control, familiarity, liking and basic emotions) felt during the videos as well as external-assessment of levels of valence and arousal. We present a detailed correlation analysis of the different dimensions as well as baseline methods and results for single-trial classification of valence and arousal, personality traits, mood and social context. The database is made publicly available.},
	number = {2},
	journal = {IEEE Transactions on Affective Computing},
	author = {Miranda-Correa, Juan Abdon and Abadi, Mojtaba Khomami and Sebe, Nicu and Patras, Ioannis},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {affect schedules, affective computing, Brain modeling, Databases, EEG, Electrocardiography, Electroencephalography, Emotion classification, Emotion recognition, mood, Mood, pattern classification, Pattern recognition, personality traits, physiological signals, Physiology, signal processing, Signal processing, Videos},
	pages = {479--493},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\DFCS-General\\Zotero\\storage\\B7B84BB7\\8554112.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\P5T6LUVC\\Miranda-Correa et al. - 2021 - AMIGOS A Dataset for Affect, Personality and Mood.pdf:application/pdf},
}

@misc{hu_unimse_2022,
	title = {{UniMSE}: {Towards} {Unified} {Multimodal} {Sentiment} {Analysis} and {Emotion} {Recognition}},
	shorttitle = {{UniMSE}},
	url = {http://arxiv.org/abs/2211.11256},
	abstract = {Multimodal sentiment analysis (MSA) and emotion recognition in conversation (ERC) are key research topics for computers to understand human behaviors. From a psychological perspective, emotions are the expression of affect or feelings during a short period, while sentiments are formed and held for a longer period. However, most existing works study sentiment and emotion separately and do not fully exploit the complementary knowledge behind the two. In this paper, we propose a multimodal sentiment knowledge-sharing framework (UniMSE) that unifies MSA and ERC tasks from features, labels, and models. We perform modality fusion at the syntactic and semantic levels and introduce contrastive learning between modalities and samples to better capture the difference and consistency between sentiments and emotions. Experiments on four public benchmark datasets, MOSI, MOSEI, MELD, and IEMOCAP, demonstrate the effectiveness of the proposed method and achieve consistent improvements compared with state-of-the-art methods.},
	urldate = {2022-12-06},
	publisher = {arXiv},
	author = {Hu, Guimin and Lin, Ting-En and Zhao, Yi and Lu, Guangming and Wu, Yuchuan and Li, Yongbin},
	month = nov,
	year = {2022},
	note = {arXiv:2211.11256 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to EMNLP 2022 main conference},
	file = {arXiv Fulltext PDF:C\:\\Users\\DFCS-General\\Zotero\\storage\\PVC53TM4\\Hu et al. - 2022 - UniMSE Towards Unified Multimodal Sentiment Analy.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\DFCS-General\\Zotero\\storage\\9EFCVTU5\\2211.html:text/html},
}
